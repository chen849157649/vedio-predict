{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total cpu count 8\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from lightgbm.sklearn import LGBMClassifier\n",
    "from sklearn.metrics import roc_auc_score, f1_score\n",
    "from scipy.stats import entropy\n",
    "from gensim.models import Word2Vec\n",
    "import time\n",
    "import gc\n",
    "import os\n",
    "\n",
    "import tqdm                                                                                                   \n",
    "import concurrent.futures\n",
    "import multiprocessing\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "num_processes = multiprocessing.cpu_count()\n",
    "print(\"total cpu count\", +num_processes) \n",
    "\n",
    "os.environ['NUMEXPR_MAX_THREADS'] = '8'\n",
    "\n",
    "from core.utils import timeit, reduce_mem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/media/ryan/F/deep-learning-data/turing/vedio-predict/\"\n",
    "\n",
    "path_sub = path + 'sub/'\n",
    "path_npy = path + 'npy/'\n",
    "path_data = path + 'raw/'\n",
    "path_model = path + 'model/'\n",
    "path_result = path + 'result/'\n",
    "path_pickle = path + 'pickle/'\n",
    "path_profile = path + 'profile/'\n",
    "\n",
    "debug_small = False\n",
    "\n",
    "if debug_small:\n",
    "    train_df = pd.read_pickle(path_pickle + 'train_small.pickle')\n",
    "    test_df = pd.read_pickle(path_pickle + 'test_small.pickle')\n",
    "    sub = pd.read_csv(path_data + 'sample.csv')\n",
    "\n",
    "    # app = pd.read_pickle(path_pickle + 'app_small.pickle')\n",
    "    # user = pd.read_pickle(path_pickle + 'user_small.pickle')\n",
    "else:\n",
    "    train_df = pd.read_pickle(path_pickle + 'train.pickle')\n",
    "    test_df = pd.read_pickle(path_pickle + 'test.pickle')\n",
    "    sub = pd.read_csv(path_data + 'sample.csv')\n",
    "\n",
    "    # app = pd.read_pickle(path_pickle + 'app.pickle')\n",
    "    # user = pd.read_pickle(path_pickle + 'user.pickle')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df = train_df[train_df.deviceid.str[-1] == '1']\n",
    "# test_df = test_df[test_df.deviceid.str[-1] == '1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sub = sub[sub.id.isin(test_df.id) ]\n",
    "# sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============================================== read train ===============================================\n",
      "runtime: 23.693913221359253\n"
     ]
    }
   ],
   "source": [
    "print('=============================================== read train ===============================================')\n",
    "t = time.time()\n",
    "# train_df = pd.read_csv('dataset/train.csv')\n",
    "train_df['date'] = pd.to_datetime(\n",
    "    train_df['ts'].apply(lambda x: time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(x / 1000)))\n",
    ")\n",
    "train_df['day'] = train_df['date'].dt.day\n",
    "\n",
    "# 训练集中，day=7的个数为11个，day=8的为3,674,871。 day9，10也是解决40w\n",
    "# day=7占比不到1/百万，属于异常情况，去掉合理？ 线上的表现又会如何，为啥不是直接删除，这样有点过了\n",
    "# 这里为啥只是改了day，不去直接改ts和timestamp呢？\n",
    "train_df.loc[train_df['day'] == 7, 'day'] = 8\n",
    "train_df['hour'] = train_df['date'].dt.hour\n",
    "train_df['minute'] = train_df['date'].dt.minute\n",
    "train_num = train_df.shape[0]\n",
    "labels = train_df['target'].values\n",
    "print('runtime:', time.time() - t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('=============================================== click data ===============================================')\n",
    "click_df = train_df[train_df['target'] == 1].sort_values('timestamp').reset_index(drop=True)\n",
    "click_df['exposure_click_gap'] = click_df['timestamp'] - click_df['ts']\n",
    "click_df = click_df[click_df['exposure_click_gap'] >= 0].reset_index(drop=True)\n",
    "click_df['date'] = pd.to_datetime(\n",
    "    click_df['timestamp'].apply(lambda x: time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(x / 1000)))\n",
    ")\n",
    "click_df['day'] = click_df['date'].dt.day\n",
    "# 同上对day==7的修改\n",
    "click_df.loc[click_df['day'] == 7, 'day'] = 8\n",
    "\n",
    "del train_df['target'], train_df['timestamp']\n",
    "\n",
    "# 这里为啥要把click_df的这些字段删除呢？\n",
    "for f in ['date', 'exposure_click_gap', 'timestamp', 'ts', 'target', 'hour', 'minute']:\n",
    "    del click_df[f]\n",
    "print('runtime:', time.time() - t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('=============================================== read test ===============================================')\n",
    "test_df['date'] = pd.to_datetime(\n",
    "    test_df['ts'].apply(lambda x: time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(x / 1000)))\n",
    ")\n",
    "test_df['day'] = test_df['date'].dt.day\n",
    "\n",
    "# 测试集中，day=10的个数为32个，day=11的为3,653,560占比 1/十万，属于异常情况，去掉合理\n",
    "test_df.loc[test_df['day'] == 10, 'day'] = 11\n",
    "test_df['hour'] = test_df['date'].dt.hour\n",
    "test_df['minute'] = test_df['date'].dt.minute\n",
    "df = pd.concat([train_df, test_df], axis=0, ignore_index=False)\n",
    "del train_df, test_df, df['date']\n",
    "gc.collect()\n",
    "print('runtime:', time.time() - t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('============================================= category encoding =============================================')\n",
    "df['lng_lat'] = df['lng'].astype('str') + '_' + df['lat'].astype('str')\n",
    "del df['guid']\n",
    "click_df['lng_lat'] = click_df['lng'].astype('str') + '_' + click_df['lat'].astype('str')\n",
    "sort_df = df.sort_values('ts').reset_index(drop=True)\n",
    "cate_cols = [\n",
    "    'deviceid', 'newsid', 'pos', 'app_version', 'device_vendor',\n",
    "    'netmodel', 'osversion', 'device_version', 'lng', 'lat', 'lng_lat'\n",
    "]\n",
    "for f in cate_cols:\n",
    "    print(f)\n",
    "    map_dict = dict(zip(df[f].unique(), range(df[f].nunique())))\n",
    "    df[f] = df[f].map(map_dict).fillna(-1).astype('int32')\n",
    "    click_df[f] = click_df[f].map(map_dict).fillna(-1).astype('int32')\n",
    "    sort_df[f] = sort_df[f].map(map_dict).fillna(-1).astype('int32')\n",
    "    df[f + '_count'] = df[f].map(df[f].value_counts())\n",
    "df = reduce_mem(df)\n",
    "click_df = reduce_mem(click_df)\n",
    "sort_df = reduce_mem(sort_df)\n",
    "print('runtime:', time.time() - t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('============================================= feat engineer =============================================')\n",
    "\n",
    "print('*************************** history stats ***************************')\n",
    "for f in [\n",
    "    ['deviceid'],\n",
    "    ['pos', 'deviceid'],\n",
    "    # ...\n",
    "]:\n",
    "    print('------------------ {} ------------------'.format('_'.join(f)))\n",
    "\n",
    "    # 对前一天的点击次数进行统计\n",
    "    tmp = click_df[f + ['day', 'id']].groupby(f + ['day'], as_index=False)['id'].agg(\n",
    "        {'_'.join(f) + '_prev_day_click_count': 'count'})\n",
    "    tmp['day'] += 1\n",
    "    df = df.merge(tmp, on=f + ['day'], how='left')\n",
    "    df['_'.join(f) + '_prev_day_click_count'] = df['_'.join(f) + '_prev_day_click_count'].fillna(0)\n",
    "    df.loc[df['day'] == 8, '_'.join(f) + '_prev_day_click_count'] = None\n",
    "\n",
    "    # 对前一天的曝光量进行统计\n",
    "    tmp = df[f + ['day', 'id']].groupby(f + ['day'], as_index=False)['id'].agg(\n",
    "        {'_'.join(f) + '_prev_day_count': 'count'})\n",
    "    tmp['day'] += 1\n",
    "    df = df.merge(tmp, on=f + ['day'], how='left')\n",
    "    df['_'.join(f) + '_prev_day_count'] = df['_'.join(f) + '_prev_day_count'].fillna(0)\n",
    "    df.loc[df['day'] == 8, '_'.join(f) + '_prev_day_count'] = None\n",
    "\n",
    "    # 计算前一天的点击率\n",
    "    df['_'.join(f) + '_prev_day_ctr'] = df['_'.join(f) + '_prev_day_click_count'] / (\n",
    "            df['_'.join(f) + '_prev_day_count'] + df['_'.join(f) + '_prev_day_count'].mean())\n",
    "\n",
    "    del tmp\n",
    "    print('runtime:', time.time() - t)\n",
    "del click_df\n",
    "df = reduce_mem(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('*************************** exposure_ts_gap ***************************')\n",
    "for f in [\n",
    "    ['deviceid'], ['newsid'], ['lng_lat'],\n",
    "    ['pos', 'deviceid'], ['pos', 'newsid'], ['pos', 'lng_lat'],\n",
    "    ['pos', 'deviceid', 'lng_lat'],\n",
    "    ['netmodel', 'deviceid'],\n",
    "    ['pos', 'netmodel', 'deviceid'],\n",
    "    ['netmodel', 'lng_lat'], ['deviceid', 'lng_lat'],\n",
    "    ['netmodel', 'deviceid', 'lng_lat'], ['pos', 'netmodel', 'lng_lat'],\n",
    "    ['pos', 'netmodel', 'deviceid', 'lng_lat']\n",
    "]:\n",
    "    print('------------------ {} ------------------'.format('_'.join(f)))\n",
    "\n",
    "    tmp = sort_df[f + ['ts']].groupby(f)\n",
    "    # 前x次、后x次曝光到当前的时间差\n",
    "    for gap in [1, 2, 3, 5, 10]:\n",
    "        sort_df['{}_prev{}_exposure_ts_gap'.format('_'.join(f), gap)] = tmp['ts'].shift(0) - tmp['ts'].shift(gap)\n",
    "        sort_df['{}_next{}_exposure_ts_gap'.format('_'.join(f), gap)] = tmp['ts'].shift(-gap) - tmp['ts'].shift(0)\n",
    "        tmp2 = sort_df[\n",
    "            f + ['ts', '{}_prev{}_exposure_ts_gap'.format('_'.join(f), gap),\n",
    "                 '{}_next{}_exposure_ts_gap'.format('_'.join(f), gap)]\n",
    "            ].drop_duplicates(f + ['ts']).reset_index(drop=True)\n",
    "        df = df.merge(tmp2, on=f + ['ts'], how='left')\n",
    "        del sort_df['{}_prev{}_exposure_ts_gap'.format('_'.join(f), gap)]\n",
    "        del sort_df['{}_next{}_exposure_ts_gap'.format('_'.join(f), gap)]\n",
    "        del tmp2\n",
    "\n",
    "    del tmp\n",
    "    df = reduce_mem(df)\n",
    "    print('runtime:', time.time() - t)\n",
    "del df['ts']\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('*************************** cross feat (second order) ***************************')\n",
    "# # 二阶交叉特征，可以继续做更高阶的交叉。\n",
    "# def build_cross_feat(df, f, col):\n",
    "#     print('------------------ {} {} ------------------'.format(f, col))\n",
    "#     df = df.merge(df[[f, col]].groupby(f, as_index=False)[col].agg({\n",
    "#         'cross_{}_{}_nunique'.format(f, col): 'nunique',\n",
    "#         'cross_{}_{}_ent'.format(f, col): lambda x: entropy(x.value_counts() / x.shape[0])  # 熵\n",
    "#     }), on=f, how='left')\n",
    "#     if 'cross_{}_{}_count'.format(f, col) not in df.columns.values and 'cross_{}_{}_count'.format(col,\n",
    "#                                                                                                   f) not in df.columns.values:\n",
    "#         df = df.merge(df[[f, col, 'id']].groupby([f, col], as_index=False)['id'].agg({\n",
    "#             'cross_{}_{}_count'.format(f, col): 'count'  # 共现次数\n",
    "#         }), on=[f, col], how='left')\n",
    "#     if 'cross_{}_{}_count_ratio'.format(col, f) not in df.columns.values:\n",
    "#         df['cross_{}_{}_count_ratio'.format(col, f)] = df['cross_{}_{}_count'.format(f, col)] / df[\n",
    "#             f + '_count']  # 比例偏好\n",
    "#     if 'cross_{}_{}_count_ratio'.format(f, col) not in df.columns.values:\n",
    "#         df['cross_{}_{}_count_ratio'.format(f, col)] = df['cross_{}_{}_count'.format(f, col)] / df[\n",
    "#             col + '_count']  # 比例偏好\n",
    "#     df['cross_{}_{}_nunique_ratio_{}_count'.format(f, col, f)] = df['cross_{}_{}_nunique'.format(f, col)] / df[\n",
    "#         f + '_count']\n",
    "#     print('runtime:', time.time() - t)\n",
    "#     df = reduce_mem(df)\n",
    "#     return df\n",
    "        \n",
    "# cross_cols = ['deviceid', 'newsid', 'pos', 'netmodel', 'lng_lat']\n",
    "# f_col_tuple_list = []\n",
    "# for f in cross_cols:\n",
    "#     for col in cross_cols:\n",
    "#         if col == f:\n",
    "#             continue\n",
    "#         f_col_tuple_list.append((f, col))\n",
    "        \n",
    "# print(f_col_tuple_list)\n",
    "# # with concurrent.futures.ProcessPoolExecutor(num_processes) as pool:\n",
    "# #     df = list(tqdm.tqdm(pool.map(build_cross_feat, cross_cols, chunksize=10, total=df.shape[0])))\n",
    "# for tuple_o in tqdm.tqdm(f_col_tuple_list):\n",
    "#     print(tuple_o)\n",
    "#     df = build_cross_feat(df, tuple_o[0], tuple_o[1])\n",
    "\n",
    "# del df['id']\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('*************************** cross feat (second order) ***************************')\n",
    "# 二阶交叉特征，可以继续做更高阶的交叉。\n",
    "cross_cols = ['deviceid', 'newsid', 'pos', 'netmodel', 'lng_lat']\n",
    "for f in cross_cols:\n",
    "    for col in cross_cols:\n",
    "        if col == f:\n",
    "            continue\n",
    "        print('------------------ {} {} ------------------'.format(f, col))\n",
    "        if  'cross_{}_{}_nunique'.format(f, col) not in df.columns.values:\n",
    "            df = df.merge(df[[f, col]].groupby(f, as_index=False)[col].agg({\n",
    "                'cross_{}_{}_nunique'.format(f, col): 'nunique',\n",
    "                'cross_{}_{}_ent'.format(f, col): lambda x: entropy(x.value_counts() / x.shape[0])  # 熵\n",
    "            }), on=f, how='left')\n",
    "        if 'cross_{}_{}_count'.format(f, col) not in df.columns.values and 'cross_{}_{}_count'.format(col,\n",
    "                                                                                                      f) not in df.columns.values:\n",
    "            df = df.merge(df[[f, col, 'id']].groupby([f, col], as_index=False)['id'].agg({\n",
    "                'cross_{}_{}_count'.format(f, col): 'count'  # 共现次数\n",
    "            }), on=[f, col], how='left')\n",
    "        if 'cross_{}_{}_count_ratio'.format(col, f) not in df.columns.values:\n",
    "            df['cross_{}_{}_count_ratio'.format(col, f)] = df['cross_{}_{}_count'.format(f, col)] / df[\n",
    "                f + '_count']  # 比例偏好\n",
    "        if 'cross_{}_{}_count_ratio'.format(f, col) not in df.columns.values:\n",
    "            df['cross_{}_{}_count_ratio'.format(f, col)] = df['cross_{}_{}_count'.format(f, col)] / df[\n",
    "                col + '_count']  # 比例偏好\n",
    "        df['cross_{}_{}_nunique_ratio_{}_count'.format(f, col, f)] = df['cross_{}_{}_nunique'.format(f, col)] / df[\n",
    "            f + '_count']\n",
    "        print('runtime:', time.time() - t)\n",
    "    df = reduce_mem(df)\n",
    "del df['id']\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle(path_pickle + \"df_081_cross.pickle\")\n",
    "print(\"success build df_081_cross.pickle\")\n",
    "\n",
    "sort_df.to_pickle(path_pickle + \"sort_df_081_cross.pickle\")\n",
    "print(\"success build sort_df_081_cross.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df =pd.read_pickle(path_pickle + \"df_081_cross.pickle\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('*************************** embedding ***************************')\n",
    "\n",
    "\n",
    "# 之前有个朋友给embedding做了一个我认为非常形象的比喻：\n",
    "# 在非诚勿扰上面，如果你想了解一个女嘉宾，那么你可以看看她都中意过哪些男嘉宾；\n",
    "# 反过来也一样，如果你想认识一个男嘉宾，那么你也可以看看他都选过哪些女嘉宾。\n",
    "\n",
    "\n",
    "def emb(df, f1, f2):\n",
    "    emb_size = 8\n",
    "    print('====================================== {} {} ======================================'.format(f1, f2))\n",
    "    tmp = df.groupby(f1, as_index=False)[f2].agg({'{}_{}_list'.format(f1, f2): list})\n",
    "    sentences = tmp['{}_{}_list'.format(f1, f2)].values.tolist()\n",
    "    del tmp['{}_{}_list'.format(f1, f2)]\n",
    "    for i in range(len(sentences)):\n",
    "        sentences[i] = [str(x) for x in sentences[i]]\n",
    "    model = Word2Vec(sentences, size=emb_size, window=5, min_count=5, sg=0, hs=1, seed=2019)\n",
    "    emb_matrix = []\n",
    "    for seq in sentences:\n",
    "        vec = []\n",
    "        for w in seq:\n",
    "            if w in model:\n",
    "                vec.append(model[w])\n",
    "        if len(vec) > 0:\n",
    "            emb_matrix.append(np.mean(vec, axis=0))\n",
    "        else:\n",
    "            emb_matrix.append([0] * emb_size)\n",
    "\n",
    "    # 为了支持数组多维处理，需要先做一个变换\n",
    "    emb_matrix = np.array(emb_matrix)\n",
    "\n",
    "    for i in range(emb_size):\n",
    "        tmp['{}_{}_emb_{}'.format(f1, f2, i)] = emb_matrix[:, i]\n",
    "    del model, emb_matrix, sentences\n",
    "    tmp = reduce_mem(tmp)\n",
    "    print('runtime:', time.time() - t)\n",
    "    return tmp\n",
    "\n",
    "\n",
    "emb_cols = [\n",
    "    ['deviceid', 'newsid'],\n",
    "    ['deviceid', 'lng_lat'],\n",
    "    ['newsid', 'lng_lat'],\n",
    "    # ...\n",
    "]\n",
    "for f1, f2 in tqdm.tqdm(emb_cols):\n",
    "    df = df.merge(emb(sort_df, f1, f2), on=f1, how='left')\n",
    "    df = df.merge(emb(sort_df, f2, f1), on=f2, how='left')\n",
    "del sort_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle(path_pickle + \"df_081_emd_all.pickle\")\n",
    "print(\"success build df_081_all.pickle\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df =pd.read_pickle(path_pickle + \"df_081_emd_all.pickle\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_num =11376681\n",
    "cate_cols = [\n",
    "    'deviceid', 'newsid', 'pos', 'app_version', 'device_vendor',\n",
    "    'netmodel', 'osversion', 'device_version', 'lng', 'lat', 'lng_lat'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('======================================== prepare train & valid  =============================================')\n",
    "train_df = df[:train_num].reset_index(drop=True)\n",
    "test_df = df[train_num:].reset_index(drop=True)\n",
    "del df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11376681, 268)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'day'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2896\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2897\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2898\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'day'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-b169b6d4f621>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'day'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mval_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'day'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtrain_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtrain_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2993\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2994\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2995\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2996\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2997\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2897\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2898\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2899\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2900\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2901\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'day'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "train_idx = train_df[train_df['day'] < 10].index.tolist()\n",
    "val_idx = train_df[train_df['day'] == 10].index.tolist()\n",
    "\n",
    "train_x = train_df.iloc[train_idx].reset_index(drop=True)\n",
    "train_y = labels[train_idx]\n",
    "val_x = train_df.iloc[val_idx].reset_index(drop=True)\n",
    "val_y = labels[val_idx]\n",
    "\n",
    "del train_x['day'], val_x['day'], train_df['day'], test_df['day']\n",
    "gc.collect()\n",
    "print('runtime:', time.time() - t)\n",
    "print('========================================================================================================')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_rate_callback(env):\n",
    "    iteration = env.iteration\n",
    "    if iteration % 10 == 0:\n",
    "        learning_rate = env.params['learning_rate'] * 0.99\n",
    "        env.params['learning_rate'] = learning_rate\n",
    "        \n",
    "        print('---- current learning rate:' + str(learning_rate) + '----')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============================================== training validate ===============================================\n",
      "************** training **************\n",
      "---- current learning rate:0.0198----\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "---- current learning rate:0.019602----\n",
      "---- current learning rate:0.01940598----\n",
      "---- current learning rate:0.0192119202----\n",
      "---- current learning rate:0.019019800998----\n",
      "[50]\tvalid_0's auc: 0.970397\n",
      "---- current learning rate:0.01882960298802----\n",
      "---- current learning rate:0.0186413069581398----\n",
      "---- current learning rate:0.0184548938885584----\n",
      "---- current learning rate:0.018270344949672817----\n",
      "---- current learning rate:0.01808764150017609----\n",
      "[100]\tvalid_0's auc: 0.973233\n",
      "---- current learning rate:0.017906765085174327----\n",
      "---- current learning rate:0.017727697434322585----\n",
      "---- current learning rate:0.01755042045997936----\n",
      "---- current learning rate:0.017374916255379565----\n",
      "---- current learning rate:0.01720116709282577----\n",
      "[150]\tvalid_0's auc: 0.975124\n",
      "---- current learning rate:0.017029155421897514----\n",
      "---- current learning rate:0.01685886386767854----\n",
      "---- current learning rate:0.016690275229001753----\n",
      "---- current learning rate:0.016523372476711734----\n",
      "---- current learning rate:0.016358138751944615----\n",
      "[200]\tvalid_0's auc: 0.976424\n",
      "---- current learning rate:0.01619455736442517----\n",
      "---- current learning rate:0.01603261179078092----\n",
      "---- current learning rate:0.015872285672873108----\n",
      "---- current learning rate:0.015713562816144378----\n",
      "---- current learning rate:0.015556427187982934----\n",
      "[250]\tvalid_0's auc: 0.977304\n",
      "---- current learning rate:0.015400862916103104----\n",
      "---- current learning rate:0.015246854286942073----\n",
      "---- current learning rate:0.015094385744072653----\n",
      "---- current learning rate:0.014943441886631926----\n",
      "---- current learning rate:0.014794007467765607----\n",
      "[300]\tvalid_0's auc: 0.977938\n",
      "---- current learning rate:0.014646067393087952----\n",
      "---- current learning rate:0.014499606719157072----\n",
      "---- current learning rate:0.014354610651965501----\n",
      "---- current learning rate:0.014211064545445845----\n",
      "---- current learning rate:0.014068953899991387----\n",
      "[350]\tvalid_0's auc: 0.978359\n",
      "---- current learning rate:0.013928264360991474----\n",
      "---- current learning rate:0.01378898171738156----\n",
      "---- current learning rate:0.013651091900207744----\n",
      "---- current learning rate:0.013514580981205667----\n",
      "---- current learning rate:0.01337943517139361----\n",
      "[400]\tvalid_0's auc: 0.978606\n",
      "---- current learning rate:0.013245640819679673----\n",
      "---- current learning rate:0.013113184411482876----\n",
      "---- current learning rate:0.012982052567368046----\n",
      "---- current learning rate:0.012852232041694365----\n",
      "---- current learning rate:0.012723709721277421----\n",
      "[450]\tvalid_0's auc: 0.978787\n",
      "---- current learning rate:0.012596472624064646----\n",
      "---- current learning rate:0.012470507897824----\n",
      "---- current learning rate:0.01234580281884576----\n",
      "---- current learning rate:0.012222344790657301----\n",
      "---- current learning rate:0.012100121342750729----\n",
      "[500]\tvalid_0's auc: 0.978913\n",
      "---- current learning rate:0.011979120129323222----\n",
      "---- current learning rate:0.011859328928029989----\n",
      "---- current learning rate:0.011740735638749689----\n",
      "---- current learning rate:0.011623328282362191----\n",
      "---- current learning rate:0.011507094999538569----\n",
      "[550]\tvalid_0's auc: 0.978999\n",
      "---- current learning rate:0.011392024049543183----\n",
      "---- current learning rate:0.01127810380904775----\n",
      "---- current learning rate:0.011165322770957272----\n",
      "---- current learning rate:0.011053669543247699----\n",
      "---- current learning rate:0.010943132847815222----\n",
      "[600]\tvalid_0's auc: 0.979066\n",
      "---- current learning rate:0.01083370151933707----\n",
      "---- current learning rate:0.010725364504143698----\n",
      "---- current learning rate:0.010618110859102262----\n",
      "---- current learning rate:0.01051192975051124----\n",
      "---- current learning rate:0.010406810453006126----\n",
      "[650]\tvalid_0's auc: 0.979125\n",
      "---- current learning rate:0.010302742348476066----\n",
      "---- current learning rate:0.010199714924991304----\n",
      "---- current learning rate:0.010097717775741392----\n",
      "---- current learning rate:0.009996740597983979----\n",
      "---- current learning rate:0.00989677319200414----\n",
      "[700]\tvalid_0's auc: 0.979155\n",
      "---- current learning rate:0.009797805460084097----\n",
      "---- current learning rate:0.009699827405483256----\n",
      "---- current learning rate:0.009602829131428424----\n",
      "---- current learning rate:0.009506800840114139----\n",
      "---- current learning rate:0.009411732831712998----\n",
      "[750]\tvalid_0's auc: 0.979212\n",
      "---- current learning rate:0.009317615503395868----\n",
      "---- current learning rate:0.00922443934836191----\n",
      "---- current learning rate:0.00913219495487829----\n",
      "---- current learning rate:0.009040873005329507----\n",
      "---- current learning rate:0.008950464275276212----\n",
      "[800]\tvalid_0's auc: 0.979235\n",
      "---- current learning rate:0.00886095963252345----\n",
      "---- current learning rate:0.008772350036198216----\n",
      "---- current learning rate:0.008684626535836234----\n",
      "---- current learning rate:0.008597780270477872----\n",
      "---- current learning rate:0.008511802467773093----\n",
      "[850]\tvalid_0's auc: 0.979281\n",
      "---- current learning rate:0.008426684443095363----\n",
      "---- current learning rate:0.00834241759866441----\n",
      "---- current learning rate:0.008258993422677765----\n",
      "---- current learning rate:0.008176403488450987----\n",
      "---- current learning rate:0.008094639453566477----\n",
      "[900]\tvalid_0's auc: 0.979299\n",
      "---- current learning rate:0.008013693059030812----\n",
      "---- current learning rate:0.007933556128440504----\n",
      "---- current learning rate:0.007854220567156098----\n",
      "---- current learning rate:0.007775678361484537----\n",
      "---- current learning rate:0.007697921577869691----\n",
      "[950]\tvalid_0's auc: 0.979302\n",
      "---- current learning rate:0.007620942362090995----\n",
      "---- current learning rate:0.007544732938470085----\n",
      "---- current learning rate:0.007469285609085384----\n",
      "---- current learning rate:0.00739459275299453----\n",
      "---- current learning rate:0.007320646825464585----\n",
      "[1000]\tvalid_0's auc: 0.979321\n",
      "---- current learning rate:0.007247440357209939----\n",
      "---- current learning rate:0.007174965953637839----\n",
      "---- current learning rate:0.007103216294101461----\n",
      "---- current learning rate:0.007032184131160447----\n",
      "---- current learning rate:0.006961862289848842----\n",
      "[1050]\tvalid_0's auc: 0.979339\n",
      "---- current learning rate:0.006892243666950354----\n",
      "---- current learning rate:0.00682332123028085----\n",
      "---- current learning rate:0.006755088017978042----\n",
      "---- current learning rate:0.006687537137798262----\n",
      "---- current learning rate:0.00662066176642028----\n",
      "[1100]\tvalid_0's auc: 0.979359\n",
      "---- current learning rate:0.006554455148756077----\n",
      "---- current learning rate:0.0064889105972685155----\n",
      "---- current learning rate:0.006424021491295831----\n",
      "---- current learning rate:0.006359781276382872----\n",
      "---- current learning rate:0.006296183463619044----\n",
      "[1150]\tvalid_0's auc: 0.97935\n",
      "---- current learning rate:0.006233221628982853----\n",
      "---- current learning rate:0.0061708894126930244----\n",
      "---- current learning rate:0.006109180518566094----\n",
      "---- current learning rate:0.006048088713380433----\n",
      "---- current learning rate:0.005987607826246628----\n",
      "[1200]\tvalid_0's auc: 0.979343\n",
      "---- current learning rate:0.005927731747984162----\n",
      "---- current learning rate:0.00586845443050432----\n",
      "---- current learning rate:0.0058097698861992765----\n",
      "---- current learning rate:0.005751672187337284----\n",
      "---- current learning rate:0.005694155465463911----\n",
      "[1250]\tvalid_0's auc: 0.979356\n",
      "---- current learning rate:0.005637213910809271----\n",
      "---- current learning rate:0.005580841771701178----\n",
      "---- current learning rate:0.005525033353984166----\n",
      "---- current learning rate:0.005469783020444325----\n",
      "---- current learning rate:0.005415085190239881----\n",
      "[1300]\tvalid_0's auc: 0.979342\n",
      "---- current learning rate:0.0053609343383374825----\n",
      "---- current learning rate:0.005307324994954107----\n",
      "Early stopping, best iteration is:\n",
      "[1113]\tvalid_0's auc: 0.979364\n",
      "runtime: 13376.071556091309\n"
     ]
    }
   ],
   "source": [
    "print('=============================================== training validate ===============================================')\n",
    "fea_imp_list = []\n",
    "clf = LGBMClassifier(\n",
    "    n_jobs=7,\n",
    "    learning_rate=0.02,\n",
    "    n_estimators=5000,\n",
    "    num_leaves=255,\n",
    "    subsample=0.9,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=2019,\n",
    "    metric=None\n",
    ")\n",
    "\n",
    "print('************** training **************')\n",
    "clf.fit(\n",
    "    train_x, train_y,\n",
    "    eval_set=[(val_x, val_y)],\n",
    "    eval_metric='auc',\n",
    "    categorical_feature=cate_cols,\n",
    "    early_stopping_rounds=200,\n",
    "    verbose=50,\n",
    "    callbacks=[learning_rate_callback]\n",
    ")\n",
    "print('runtime:', time.time() - t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************** validate predict **************\n",
      "runtime: 16980.81683063507\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "\n",
    "print('************** validate predict **************')\n",
    "best_rounds = clf.best_iteration_\n",
    "best_auc = clf.best_score_['valid_0']['auc']\n",
    "val_pred = clf.predict_proba(val_x)[:, 1]\n",
    "fea_imp_list.append(clf.feature_importances_)\n",
    "print('runtime:', time.time() - t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print('=============================================== training predict ===============================================')\n",
    "clf = LGBMClassifier(\n",
    "    learning_rate=0.01,\n",
    "    n_estimators=best_rounds,\n",
    "    num_leaves=255,\n",
    "    subsample=0.9,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=2019\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('************** training using all the data **************')\n",
    "clf.fit(\n",
    "    train_df, labels,\n",
    "    eval_set=[(train_df, labels)],\n",
    "    categorical_feature=cate_cols,\n",
    "    verbose=50\n",
    ")\n",
    "print('runtime:', time.time() - t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************** test predict **************\n",
      "runtime: 25565.922644615173\n"
     ]
    }
   ],
   "source": [
    "print('************** test predict **************')\n",
    "# sub = pd.read_csv(path_data + 'sample.csv')\n",
    "\n",
    "sub['target'] = clf.predict_proba(test_df)[:, 1]\n",
    "clf.predict_proba(test_df)[:, 1]\n",
    "fea_imp_list.append(clf.feature_importances_)\n",
    "print('runtime:', time.time() - t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============================================== feat importances ===============================================\n",
      "deviceid = 87117.0\n",
      "newsid = 43714.0\n",
      "device_version = 33658.0\n",
      "lat = 15132.0\n",
      "lng = 14444.0\n",
      "lng_lat = 10186.0\n",
      "netmodel_deviceid_lng_lat_next1_exposure_ts_gap = 3269.0\n",
      "netmodel_deviceid_next1_exposure_ts_gap = 2787.0\n",
      "deviceid_next3_exposure_ts_gap = 2426.0\n",
      "netmodel_deviceid_lng_lat_next2_exposure_ts_gap = 1942.0\n",
      "netmodel_deviceid_next3_exposure_ts_gap = 1800.0\n",
      "pos_netmodel_deviceid_lng_lat_next1_exposure_ts_gap = 1763.0\n",
      "pos = 1741.0\n",
      "deviceid_lng_lat_next3_exposure_ts_gap = 1735.0\n",
      "netmodel_deviceid_next2_exposure_ts_gap = 1734.0\n",
      "deviceid_next1_exposure_ts_gap = 1696.0\n",
      "pos_netmodel_deviceid_next1_exposure_ts_gap = 1682.0\n",
      "cross_deviceid_newsid_count = 1579.0\n",
      "pos_count = 1561.0\n",
      "cross_lng_lat_pos_ent = 1536.0\n",
      "cross_deviceid_pos_ent_x = 1458.0\n",
      "netmodel_deviceid_lng_lat_next3_exposure_ts_gap = 1447.0\n",
      "deviceid_next2_exposure_ts_gap = 1191.0\n",
      "netmodel_deviceid_next10_exposure_ts_gap = 1010.0\n",
      "deviceid_next5_exposure_ts_gap = 998.0\n",
      "deviceid_lng_lat_next1_exposure_ts_gap = 993.0\n",
      "netmodel_deviceid_next5_exposure_ts_gap = 893.0\n",
      "deviceid_lng_lat_next2_exposure_ts_gap = 816.0\n",
      "netmodel_deviceid_lng_lat_next10_exposure_ts_gap = 788.0\n",
      "pos_netmodel_deviceid_next2_exposure_ts_gap = 727.0\n",
      "pos_deviceid_prev_day_ctr = 683.0\n",
      "pos_deviceid_next1_exposure_ts_gap = 683.0\n",
      "deviceid_lng_lat_next5_exposure_ts_gap = 668.0\n",
      "cross_pos_netmodel_count = 609.0\n",
      "pos_deviceid_lng_lat_next1_exposure_ts_gap = 600.0\n",
      "cross_netmodel_pos_count_ratio = 600.0\n",
      "cross_pos_deviceid_count_ratio = 596.0\n",
      "cross_newsid_deviceid_count_ratio = 583.0\n",
      "pos_netmodel_deviceid_lng_lat_next2_exposure_ts_gap = 578.0\n",
      "cross_pos_lng_lat_count_ratio = 559.0\n",
      "netmodel_deviceid_lng_lat_next5_exposure_ts_gap = 549.0\n",
      "cross_netmodel_deviceid_count_ratio = 517.0\n",
      "netmodel_lng_lat_next1_exposure_ts_gap = 513.0\n",
      "pos_netmodel_deviceid_next3_exposure_ts_gap = 510.0\n",
      "cross_pos_netmodel_count_ratio = 500.0\n",
      "netmodel_count = 466.0\n",
      "deviceid_next10_exposure_ts_gap = 461.0\n",
      "cross_pos_netmodel_ent = 432.0\n",
      "cross_deviceid_pos_nunique_x = 429.0\n",
      "deviceid_prev_day_ctr = 424.0\n",
      "cross_lng_lat_pos_nunique = 420.0\n",
      "newsid_next10_exposure_ts_gap = 415.0\n",
      "netmodel_deviceid_prev5_exposure_ts_gap = 414.0\n",
      "netmodel_deviceid_prev1_exposure_ts_gap = 409.0\n",
      "cross_deviceid_netmodel_count_ratio = 408.0\n",
      "cross_netmodel_newsid_count_ratio = 406.0\n",
      "cross_netmodel_lng_lat_count_ratio = 387.0\n",
      "pos_newsid_next1_exposure_ts_gap = 382.0\n",
      "pos_netmodel_deviceid_prev1_exposure_ts_gap = 378.0\n",
      "pos_deviceid_lng_lat_next2_exposure_ts_gap = 376.0\n",
      "netmodel_deviceid_prev10_exposure_ts_gap = 367.0\n",
      "cross_newsid_pos_ent = 364.0\n",
      "pos_deviceid_next2_exposure_ts_gap = 347.0\n",
      "newsid_next1_exposure_ts_gap = 336.0\n",
      "deviceid_prev2_exposure_ts_gap = 318.0\n",
      "newsid_prev1_exposure_ts_gap = 318.0\n",
      "pos_newsid_prev1_exposure_ts_gap = 315.0\n",
      "cross_newsid_netmodel_count_ratio = 310.0\n",
      "deviceid_lng_lat_next10_exposure_ts_gap = 300.0\n",
      "pos_netmodel_deviceid_next5_exposure_ts_gap = 299.0\n",
      "deviceid_prev1_exposure_ts_gap = 289.0\n",
      "cross_deviceid_pos_count_ratio = 281.0\n",
      "cross_pos_newsid_ent = 281.0\n",
      "pos_deviceid_next3_exposure_ts_gap = 279.0\n",
      "cross_deviceid_pos_nunique_ratio_deviceid_count = 279.0\n",
      "cross_deviceid_newsid_count_ratio = 278.0\n",
      "pos_netmodel_lng_lat_next1_exposure_ts_gap = 277.0\n",
      "cross_lng_lat_pos_count_ratio = 270.0\n",
      "cross_deviceid_netmodel_ent = 269.0\n",
      "cross_deviceid_pos_ent_y = 269.0\n",
      "netmodel_lng_lat_next2_exposure_ts_gap = 264.0\n",
      "cross_deviceid_netmodel_count = 257.0\n",
      "netmodel_deviceid_prev3_exposure_ts_gap = 256.0\n",
      "netmodel_deviceid_lng_lat_prev2_exposure_ts_gap = 252.0\n",
      "lng_lat_next3_exposure_ts_gap = 250.0\n",
      "deviceid_lng_lat_prev1_exposure_ts_gap = 249.0\n",
      "pos_netmodel_deviceid_lng_lat_next3_exposure_ts_gap = 241.0\n",
      "cross_newsid_deviceid_nunique_ratio_newsid_count = 240.0\n",
      "netmodel_deviceid_prev2_exposure_ts_gap = 233.0\n",
      "netmodel_deviceid_lng_lat_prev1_exposure_ts_gap = 232.0\n",
      "cross_pos_deviceid_nunique = 230.0\n",
      "pos_deviceid_prev1_exposure_ts_gap = 227.0\n",
      "cross_newsid_lng_lat_count = 217.0\n",
      "netmodel_lng_lat_next10_exposure_ts_gap = 211.0\n",
      "deviceid_prev_day_count = 207.0\n",
      "pos_newsid_next10_exposure_ts_gap = 207.0\n",
      "lng_lat_next1_exposure_ts_gap = 206.0\n",
      "pos_netmodel_deviceid_next10_exposure_ts_gap = 205.0\n",
      "deviceid_prev5_exposure_ts_gap = 202.0\n",
      "newsid_next5_exposure_ts_gap = 200.0\n",
      "newsid_prev10_exposure_ts_gap = 200.0\n",
      "deviceid_prev10_exposure_ts_gap = 199.0\n",
      "netmodel_lng_lat_next3_exposure_ts_gap = 197.0\n",
      "cross_newsid_netmodel_count = 196.0\n",
      "cross_newsid_pos_count_ratio = 195.0\n",
      "cross_newsid_netmodel_nunique_ratio_newsid_count = 194.0\n",
      "cross_deviceid_lng_lat_nunique_ratio_deviceid_count = 193.0\n",
      "pos_newsid_next5_exposure_ts_gap = 192.0\n",
      "cross_newsid_pos_nunique_ratio_newsid_count = 192.0\n",
      "pos_deviceid_next5_exposure_ts_gap = 191.0\n",
      "cross_lng_lat_netmodel_count_ratio = 189.0\n",
      "netmodel_deviceid_lng_lat_prev3_exposure_ts_gap = 188.0\n",
      "cross_lng_lat_pos_nunique_ratio_lng_lat_count = 182.0\n",
      "cross_lng_lat_netmodel_ent = 182.0\n",
      "newsid_next3_exposure_ts_gap = 181.0\n",
      "device_version_count = 180.0\n",
      "newsid_next2_exposure_ts_gap = 180.0\n",
      "deviceid_lng_lat_prev2_exposure_ts_gap = 177.0\n",
      "pos_deviceid_prev_day_count = 175.0\n",
      "cross_newsid_netmodel_ent = 171.0\n",
      "pos_netmodel_deviceid_lng_lat_prev1_exposure_ts_gap = 170.0\n",
      "cross_pos_lng_lat_nunique_ratio_pos_count = 167.0\n",
      "lng_lat_next5_exposure_ts_gap = 166.0\n",
      "pos_netmodel_deviceid_prev3_exposure_ts_gap = 166.0\n",
      "newsid_prev5_exposure_ts_gap = 165.0\n",
      "cross_newsid_lng_lat_nunique_ratio_newsid_count = 165.0\n",
      "pos_netmodel_deviceid_prev2_exposure_ts_gap = 157.0\n",
      "pos_netmodel_deviceid_prev5_exposure_ts_gap = 157.0\n",
      "deviceid_prev3_exposure_ts_gap = 156.0\n",
      "pos_deviceid_prev3_exposure_ts_gap = 156.0\n",
      "cross_netmodel_deviceid_ent = 154.0\n",
      "netmodel_lng_lat_next5_exposure_ts_gap = 153.0\n",
      "cross_netmodel_lng_lat_count = 153.0\n",
      "pos_deviceid_prev2_exposure_ts_gap = 152.0\n",
      "pos_deviceid_prev5_exposure_ts_gap = 151.0\n",
      "pos_deviceid_lng_lat_prev1_exposure_ts_gap = 150.0\n",
      "netmodel_deviceid_lng_lat_prev5_exposure_ts_gap = 150.0\n",
      "pos_newsid_next3_exposure_ts_gap = 143.0\n",
      "cross_pos_deviceid_ent = 143.0\n",
      "newsid_prev3_exposure_ts_gap = 142.0\n",
      "pos_deviceid_next10_exposure_ts_gap = 142.0\n",
      "cross_newsid_lng_lat_ent = 142.0\n",
      "newsid_count = 139.0\n",
      "cross_newsid_lng_lat_count_ratio = 137.0\n",
      "lng_lat_next2_exposure_ts_gap = 135.0\n",
      "cross_deviceid_pos_count = 135.0\n",
      "cross_deviceid_netmodel_nunique_ratio_deviceid_count = 135.0\n",
      "pos_netmodel_deviceid_prev10_exposure_ts_gap = 134.0\n",
      "pos_lng_lat_next1_exposure_ts_gap = 131.0\n",
      "pos_lng_lat_next2_exposure_ts_gap = 124.0\n",
      "cross_deviceid_newsid_ent_x = 124.0\n",
      "cross_pos_newsid_count_ratio = 124.0\n",
      "newsid_prev2_exposure_ts_gap = 122.0\n",
      "pos_deviceid_prev10_exposure_ts_gap = 117.0\n",
      "app_version = 115.0\n",
      "pos_newsid_prev2_exposure_ts_gap = 113.0\n",
      "pos_deviceid_lng_lat_next3_exposure_ts_gap = 113.0\n",
      "netmodel_lng_lat_prev1_exposure_ts_gap = 113.0\n",
      "deviceid_count = 111.0\n",
      "pos_newsid_next2_exposure_ts_gap = 105.0\n",
      "pos_newsid_prev10_exposure_ts_gap = 105.0\n",
      "pos_netmodel_deviceid_lng_lat_next5_exposure_ts_gap = 105.0\n",
      "lng_lat_prev1_exposure_ts_gap = 102.0\n",
      "lng_lat_next10_exposure_ts_gap = 102.0\n",
      "cross_deviceid_newsid_nunique_ratio_deviceid_count = 102.0\n",
      "pos_deviceid_lng_lat_next5_exposure_ts_gap = 101.0\n",
      "cross_newsid_pos_count = 100.0\n",
      "cross_deviceid_pos_nunique_y = 99.0\n",
      "pos_netmodel_lng_lat_next2_exposure_ts_gap = 98.0\n",
      "pos_netmodel_deviceid_lng_lat_prev2_exposure_ts_gap = 98.0\n",
      "device_vendor = 97.0\n",
      "pos_newsid_prev3_exposure_ts_gap = 95.0\n",
      "pos_netmodel_deviceid_lng_lat_prev3_exposure_ts_gap = 95.0\n",
      "pos_netmodel_lng_lat_next3_exposure_ts_gap = 94.0\n",
      "netmodel_lng_lat_prev5_exposure_ts_gap = 93.0\n",
      "cross_newsid_deviceid_ent = 92.0\n",
      "cross_newsid_lng_lat_nunique = 91.0\n",
      "netmodel_deviceid_lng_lat_prev10_exposure_ts_gap = 89.0\n",
      "pos_newsid_prev5_exposure_ts_gap = 87.0\n",
      "deviceid_prev_day_click_count = 86.0\n",
      "netmodel_lng_lat_prev2_exposure_ts_gap = 86.0\n",
      "pos_netmodel_lng_lat_prev1_exposure_ts_gap = 83.0\n",
      "minute = 82.0\n",
      "cross_deviceid_lng_lat_ent = 82.0\n",
      "pos_lng_lat_next5_exposure_ts_gap = 80.0\n",
      "pos_netmodel_deviceid_lng_lat_next10_exposure_ts_gap = 80.0\n",
      "cross_deviceid_newsid_nunique_x = 79.0\n",
      "cross_lng_lat_newsid_count_ratio = 79.0\n",
      "pos_lng_lat_next3_exposure_ts_gap = 78.0\n",
      "lng_lat_prev2_exposure_ts_gap = 77.0\n",
      "pos_lng_lat_prev1_exposure_ts_gap = 77.0\n",
      "cross_lng_lat_deviceid_nunique_ratio_lng_lat_count = 77.0\n",
      "cross_lng_lat_newsid_nunique_ratio_lng_lat_count = 77.0\n",
      "cross_deviceid_lng_lat_count = 76.0\n",
      "netmodel_lng_lat_prev10_exposure_ts_gap = 74.0\n",
      "cross_lng_lat_deviceid_count_ratio = 72.0\n",
      "pos_netmodel_deviceid_lng_lat_prev10_exposure_ts_gap = 71.0\n",
      "cross_newsid_deviceid_nunique = 71.0\n",
      "netmodel_lng_lat_prev3_exposure_ts_gap = 70.0\n",
      "deviceid_lng_lat_prev3_exposure_ts_gap = 70.0\n",
      "pos_netmodel_lng_lat_next5_exposure_ts_gap = 70.0\n",
      "pos_netmodel_lng_lat_prev2_exposure_ts_gap = 69.0\n",
      "cross_pos_deviceid_nunique_ratio_pos_count = 69.0\n",
      "hour = 68.0\n",
      "cross_newsid_pos_nunique = 68.0\n",
      "cross_lng_lat_newsid_nunique = 68.0\n",
      "pos_deviceid_prev_day_click_count = 67.0\n",
      "pos_deviceid_lng_lat_prev10_exposure_ts_gap = 67.0\n",
      "pos_deviceid_lng_lat_prev3_exposure_ts_gap = 66.0\n",
      "pos_netmodel_lng_lat_prev3_exposure_ts_gap = 66.0\n",
      "cross_lng_lat_newsid_ent = 65.0\n",
      "app_version_count = 64.0\n",
      "deviceid_lng_lat_prev5_exposure_ts_gap = 64.0\n",
      "lng_lat_prev3_exposure_ts_gap = 63.0\n",
      "pos_lng_lat_prev10_exposure_ts_gap = 62.0\n",
      "pos_netmodel_lng_lat_prev10_exposure_ts_gap = 61.0\n",
      "device_vendor_count = 60.0\n",
      "pos_lng_lat_prev5_exposure_ts_gap = 60.0\n",
      "pos_deviceid_lng_lat_next10_exposure_ts_gap = 60.0\n",
      "lat_count = 59.0\n",
      "pos_netmodel_deviceid_lng_lat_prev5_exposure_ts_gap = 58.0\n",
      "lng_lat_prev5_exposure_ts_gap = 57.0\n",
      "cross_pos_lng_lat_count = 56.0\n",
      "pos_lng_lat_next10_exposure_ts_gap = 55.0\n",
      "pos_netmodel_lng_lat_prev5_exposure_ts_gap = 54.0\n",
      "cross_deviceid_pos_ent = 53.0\n",
      "lng_count = 50.0\n",
      "deviceid_lng_lat_prev10_exposure_ts_gap = 50.0\n",
      "pos_lng_lat_prev2_exposure_ts_gap = 49.0\n",
      "pos_netmodel_lng_lat_next10_exposure_ts_gap = 49.0\n",
      "lng_lat_prev10_exposure_ts_gap = 48.0\n",
      "pos_deviceid_lng_lat_prev5_exposure_ts_gap = 48.0\n",
      "cross_deviceid_lng_lat_nunique = 48.0\n",
      "cross_deviceid_lng_lat_count_ratio = 48.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross_lng_lat_netmodel_nunique_ratio_lng_lat_count = 48.0\n",
      "osversion_count = 46.0\n",
      "pos_lng_lat_prev3_exposure_ts_gap = 43.0\n",
      "pos_deviceid_lng_lat_prev2_exposure_ts_gap = 42.0\n",
      "netmodel = 24.0\n",
      "cross_deviceid_netmodel_nunique = 24.0\n",
      "cross_deviceid_newsid_ent_y = 24.0\n",
      "cross_netmodel_deviceid_nunique_ratio_netmodel_count = 23.0\n",
      "cross_newsid_netmodel_nunique = 19.0\n",
      "cross_pos_lng_lat_ent = 18.0\n",
      "cross_deviceid_pos_nunique = 13.0\n",
      "cross_pos_newsid_nunique_ratio_pos_count = 13.0\n",
      "cross_netmodel_deviceid_nunique = 13.0\n",
      "cross_lng_lat_netmodel_nunique = 13.0\n",
      "cross_netmodel_lng_lat_ent = 12.0\n",
      "osversion = 11.0\n",
      "lng_lat_count = 11.0\n",
      "cross_netmodel_newsid_nunique_ratio_netmodel_count = 5.0\n",
      "cross_deviceid_newsid_nunique_y = 4.0\n",
      "cross_netmodel_lng_lat_nunique_ratio_netmodel_count = 4.0\n",
      "cross_deviceid_newsid_ent = 3.0\n",
      "cross_netmodel_pos_nunique_ratio_netmodel_count = 3.0\n",
      "cross_pos_newsid_nunique = 1.0\n",
      "cross_pos_netmodel_nunique_ratio_pos_count = 1.0\n",
      "cross_netmodel_newsid_nunique = 1.0\n",
      "cross_lng_lat_deviceid_nunique = 1.0\n",
      "cross_deviceid_newsid_nunique = 0.0\n",
      "cross_pos_netmodel_nunique = 0.0\n",
      "cross_pos_lng_lat_nunique = 0.0\n",
      "cross_netmodel_newsid_ent = 0.0\n",
      "cross_netmodel_pos_nunique = 0.0\n",
      "cross_netmodel_pos_ent = 0.0\n",
      "cross_netmodel_lng_lat_nunique = 0.0\n",
      "cross_lng_lat_deviceid_ent = 0.0\n"
     ]
    }
   ],
   "source": [
    "print('=============================================== feat importances ===============================================')\n",
    "# 特征重要性可以好好看看\n",
    "fea_imp_dict = dict(zip(train_df.columns.values, np.mean(fea_imp_list, axis=0)))\n",
    "fea_imp_item = sorted(fea_imp_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "for f, imp in fea_imp_item:\n",
    "    print('{} = {}'.format(f, imp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============================================== threshold search ===============================================\n",
      "step: 0   best threshold: 0.05   best f1: 0.6741247259698413\n",
      "step: 1   best threshold: 0.052000000000000005   best f1: 0.677214481657178\n",
      "step: 2   best threshold: 0.054000000000000006   best f1: 0.6802106795934408\n",
      "step: 3   best threshold: 0.056   best f1: 0.6831118601178485\n",
      "step: 4   best threshold: 0.058   best f1: 0.6859091355624608\n",
      "step: 5   best threshold: 0.060000000000000005   best f1: 0.688564232436159\n",
      "step: 6   best threshold: 0.062   best f1: 0.6912090360858483\n",
      "step: 7   best threshold: 0.064   best f1: 0.6937056551492506\n",
      "step: 8   best threshold: 0.066   best f1: 0.6961010887426297\n",
      "step: 9   best threshold: 0.068   best f1: 0.698368284377157\n",
      "step: 10   best threshold: 0.07   best f1: 0.700670622072013\n",
      "step: 11   best threshold: 0.07200000000000001   best f1: 0.7028456949514735\n",
      "step: 12   best threshold: 0.07400000000000001   best f1: 0.705000116236027\n",
      "step: 13   best threshold: 0.07600000000000001   best f1: 0.7070492886434312\n",
      "step: 14   best threshold: 0.078   best f1: 0.7090458112965387\n",
      "step: 15   best threshold: 0.08   best f1: 0.7109801799780321\n",
      "step: 16   best threshold: 0.082   best f1: 0.7129212329949118\n",
      "step: 17   best threshold: 0.084   best f1: 0.7146962031086194\n",
      "step: 18   best threshold: 0.08600000000000001   best f1: 0.7164558421134968\n",
      "step: 19   best threshold: 0.088   best f1: 0.7181600103555879\n",
      "step: 20   best threshold: 0.09   best f1: 0.7198704330506274\n",
      "step: 21   best threshold: 0.092   best f1: 0.7215537146691858\n",
      "step: 22   best threshold: 0.094   best f1: 0.7231653088383264\n",
      "step: 23   best threshold: 0.096   best f1: 0.7246746794707792\n",
      "step: 24   best threshold: 0.098   best f1: 0.7261463454696815\n",
      "step: 25   best threshold: 0.1   best f1: 0.7275980126442559\n",
      "step: 26   best threshold: 0.10200000000000001   best f1: 0.7290515489476471\n",
      "step: 27   best threshold: 0.10400000000000001   best f1: 0.7304346024189895\n",
      "step: 28   best threshold: 0.10600000000000001   best f1: 0.7317768611438105\n",
      "step: 29   best threshold: 0.10800000000000001   best f1: 0.733075512168502\n",
      "step: 30   best threshold: 0.11   best f1: 0.7343396972167274\n",
      "step: 31   best threshold: 0.112   best f1: 0.7356812495116712\n",
      "step: 32   best threshold: 0.114   best f1: 0.7369100930895152\n",
      "step: 33   best threshold: 0.116   best f1: 0.7381966765504722\n",
      "step: 34   best threshold: 0.11800000000000001   best f1: 0.7394214788698569\n",
      "step: 35   best threshold: 0.12000000000000001   best f1: 0.7405986215292626\n",
      "step: 36   best threshold: 0.12200000000000001   best f1: 0.7417836204507288\n",
      "step: 37   best threshold: 0.124   best f1: 0.7429131845747524\n",
      "step: 38   best threshold: 0.126   best f1: 0.7439709456259269\n",
      "step: 39   best threshold: 0.128   best f1: 0.7450166953354245\n",
      "step: 40   best threshold: 0.13   best f1: 0.7460625198702558\n",
      "step: 41   best threshold: 0.132   best f1: 0.7470948505511158\n",
      "step: 42   best threshold: 0.134   best f1: 0.7481041100561785\n",
      "step: 43   best threshold: 0.136   best f1: 0.7490292954841244\n",
      "step: 44   best threshold: 0.138   best f1: 0.7500226453734516\n",
      "step: 45   best threshold: 0.14   best f1: 0.7510159853627053\n",
      "step: 46   best threshold: 0.14200000000000002   best f1: 0.7519605127799319\n",
      "step: 47   best threshold: 0.14400000000000002   best f1: 0.7528598152706552\n",
      "step: 48   best threshold: 0.14600000000000002   best f1: 0.7537810174591046\n",
      "step: 49   best threshold: 0.14800000000000002   best f1: 0.754664563654939\n",
      "step: 50   best threshold: 0.15000000000000002   best f1: 0.7555842940264892\n",
      "step: 51   best threshold: 0.15200000000000002   best f1: 0.7564041285605434\n",
      "step: 52   best threshold: 0.15400000000000003   best f1: 0.7572152624269255\n",
      "step: 53   best threshold: 0.156   best f1: 0.75808628314141\n",
      "step: 54   best threshold: 0.158   best f1: 0.7589684147189363\n",
      "step: 55   best threshold: 0.16   best f1: 0.7597776290106628\n",
      "step: 56   best threshold: 0.162   best f1: 0.7606209785857957\n",
      "step: 57   best threshold: 0.164   best f1: 0.7614206106986138\n",
      "step: 58   best threshold: 0.166   best f1: 0.7621585325776048\n",
      "step: 59   best threshold: 0.168   best f1: 0.7628650374018208\n",
      "step: 60   best threshold: 0.16999999999999998   best f1: 0.7636118148271845\n",
      "step: 61   best threshold: 0.172   best f1: 0.7642691798773116\n",
      "step: 62   best threshold: 0.174   best f1: 0.7649771846327461\n",
      "step: 63   best threshold: 0.176   best f1: 0.7656431463877742\n",
      "step: 64   best threshold: 0.178   best f1: 0.7663975637084494\n",
      "step: 65   best threshold: 0.18   best f1: 0.767092391639562\n",
      "step: 66   best threshold: 0.182   best f1: 0.7677208332818402\n",
      "step: 67   best threshold: 0.184   best f1: 0.7683997647471548\n",
      "step: 68   best threshold: 0.186   best f1: 0.7689931935522215\n",
      "step: 69   best threshold: 0.188   best f1: 0.7696396989690893\n",
      "step: 70   best threshold: 0.19   best f1: 0.7702715598614195\n",
      "step: 71   best threshold: 0.192   best f1: 0.7708700052578967\n",
      "step: 72   best threshold: 0.194   best f1: 0.7714489727517431\n",
      "step: 73   best threshold: 0.196   best f1: 0.7720467867964308\n",
      "step: 74   best threshold: 0.198   best f1: 0.7725347707070696\n",
      "step: 75   best threshold: 0.2   best f1: 0.7731377447472706\n",
      "step: 76   best threshold: 0.202   best f1: 0.7737123011901605\n",
      "step: 77   best threshold: 0.20400000000000001   best f1: 0.7742416290542667\n",
      "step: 78   best threshold: 0.20600000000000002   best f1: 0.774800463893191\n",
      "step: 79   best threshold: 0.20800000000000002   best f1: 0.7753796120574352\n",
      "step: 80   best threshold: 0.21000000000000002   best f1: 0.7758518654484576\n",
      "step: 81   best threshold: 0.21200000000000002   best f1: 0.776312091209755\n",
      "step: 82   best threshold: 0.21400000000000002   best f1: 0.7767550891898042\n",
      "step: 83   best threshold: 0.21600000000000003   best f1: 0.7772857167085002\n",
      "step: 84   best threshold: 0.21800000000000003   best f1: 0.7777836757833319\n",
      "step: 85   best threshold: 0.22000000000000003   best f1: 0.7782275686728616\n",
      "step: 86   best threshold: 0.22200000000000003   best f1: 0.7787081314233983\n",
      "step: 87   best threshold: 0.22400000000000003   best f1: 0.7791716844819108\n",
      "step: 88   best threshold: 0.22599999999999998   best f1: 0.779588191430946\n",
      "step: 89   best threshold: 0.22799999999999998   best f1: 0.7799793524367868\n",
      "step: 90   best threshold: 0.22999999999999998   best f1: 0.7804218971657803\n",
      "step: 91   best threshold: 0.23199999999999998   best f1: 0.7808587856298169\n",
      "step: 92   best threshold: 0.23399999999999999   best f1: 0.7812159790562637\n",
      "step: 93   best threshold: 0.236   best f1: 0.7815762622109519\n",
      "step: 94   best threshold: 0.238   best f1: 0.7819348996652814\n",
      "step: 95   best threshold: 0.24   best f1: 0.7822919736553761\n",
      "step: 96   best threshold: 0.242   best f1: 0.7827198159501907\n",
      "step: 97   best threshold: 0.244   best f1: 0.7830426046060438\n",
      "step: 98   best threshold: 0.246   best f1: 0.7833922610122118\n",
      "step: 99   best threshold: 0.248   best f1: 0.7837264401913668\n",
      "step: 100   best threshold: 0.25   best f1: 0.7840646010876315\n",
      "step: 101   best threshold: 0.252   best f1: 0.784377991029769\n",
      "step: 102   best threshold: 0.254   best f1: 0.7846842995969782\n",
      "step: 103   best threshold: 0.256   best f1: 0.7849860284105366\n",
      "step: 104   best threshold: 0.258   best f1: 0.7853006399013248\n",
      "step: 105   best threshold: 0.26   best f1: 0.7856222275998831\n",
      "step: 106   best threshold: 0.262   best f1: 0.785910402722831\n",
      "step: 107   best threshold: 0.264   best f1: 0.7862454266351013\n",
      "step: 108   best threshold: 0.266   best f1: 0.7865045987058752\n",
      "step: 109   best threshold: 0.268   best f1: 0.7867429859879399\n",
      "step: 110   best threshold: 0.27   best f1: 0.7870938438115649\n",
      "step: 111   best threshold: 0.272   best f1: 0.787375862723712\n",
      "step: 112   best threshold: 0.274   best f1: 0.7877120475388834\n",
      "step: 113   best threshold: 0.276   best f1: 0.7879648517355534\n",
      "step: 114   best threshold: 0.278   best f1: 0.788155119655723\n",
      "step: 115   best threshold: 0.28   best f1: 0.7885150813352315\n",
      "step: 116   best threshold: 0.28200000000000003   best f1: 0.7887244754529887\n",
      "step: 117   best threshold: 0.28400000000000003   best f1: 0.7889733713393101\n",
      "step: 118   best threshold: 0.28600000000000003   best f1: 0.7892248390187385\n",
      "step: 119   best threshold: 0.28800000000000003   best f1: 0.7894299447714727\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 120   best threshold: 0.29   best f1: 0.7897447697856753\n",
      "step: 121   best threshold: 0.292   best f1: 0.7899719267635249\n",
      "step: 122   best threshold: 0.294   best f1: 0.7902039481641082\n",
      "step: 123   best threshold: 0.296   best f1: 0.7903516433983188\n",
      "step: 124   best threshold: 0.298   best f1: 0.7905416936121291\n",
      "step: 125   best threshold: 0.3   best f1: 0.790703687631673\n",
      "step: 126   best threshold: 0.302   best f1: 0.7908918585489288\n",
      "step: 127   best threshold: 0.304   best f1: 0.7911330704375198\n",
      "step: 128   best threshold: 0.306   best f1: 0.791313552149607\n",
      "step: 129   best threshold: 0.308   best f1: 0.7914395242975015\n",
      "step: 130   best threshold: 0.31   best f1: 0.7916364529021616\n",
      "step: 131   best threshold: 0.312   best f1: 0.7917843720479263\n",
      "step: 132   best threshold: 0.314   best f1: 0.7919713947768023\n",
      "step: 133   best threshold: 0.316   best f1: 0.7921508162324553\n",
      "step: 134   best threshold: 0.318   best f1: 0.7923015710615464\n",
      "step: 135   best threshold: 0.32   best f1: 0.792442178109634\n",
      "step: 136   best threshold: 0.322   best f1: 0.7925648964744683\n",
      "step: 137   best threshold: 0.324   best f1: 0.7926839007938591\n",
      "step: 138   best threshold: 0.326   best f1: 0.7927685877247392\n",
      "step: 139   best threshold: 0.328   best f1: 0.7928529439347465\n",
      "step: 140   best threshold: 0.33   best f1: 0.792960954111549\n",
      "step: 141   best threshold: 0.332   best f1: 0.7930518235925277\n",
      "step: 142   best threshold: 0.334   best f1: 0.7932392788441672\n",
      "step: 143   best threshold: 0.336   best f1: 0.7933715446631304\n",
      "step: 144   best threshold: 0.338   best f1: 0.7934864767540989\n",
      "step: 145   best threshold: 0.33999999999999997   best f1: 0.7935831488550058\n",
      "step: 146   best threshold: 0.34199999999999997   best f1: 0.7936986772111537\n",
      "step: 147   best threshold: 0.344   best f1: 0.7938305015863633\n",
      "step: 148   best threshold: 0.346   best f1: 0.7939075122727078\n",
      "step: 149   best threshold: 0.348   best f1: 0.7940193837697093\n",
      "step: 150   best threshold: 0.35   best f1: 0.7941089919897677\n",
      "step: 151   best threshold: 0.352   best f1: 0.7941661920762932\n",
      "step: 152   best threshold: 0.354   best f1: 0.7942353474004481\n",
      "step: 153   best threshold: 0.356   best f1: 0.7943740961026036\n",
      "step: 154   best threshold: 0.358   best f1: 0.7944527212626287\n",
      "step: 155   best threshold: 0.36   best f1: 0.7945165571865892\n",
      "step: 156   best threshold: 0.362   best f1: 0.7945375210005601\n",
      "step: 158   best threshold: 0.366   best f1: 0.7945448635046549\n",
      "step: 159   best threshold: 0.368   best f1: 0.7945810028399048\n",
      "step: 161   best threshold: 0.372   best f1: 0.7946016151040267\n",
      "step: 162   best threshold: 0.374   best f1: 0.7946043469681167\n",
      "step: 163   best threshold: 0.376   best f1: 0.7946486422287838\n",
      "step: 165   best threshold: 0.38   best f1: 0.7946546225163064\n",
      "step: 166   best threshold: 0.382   best f1: 0.7946912658154661\n",
      "step: 170   best threshold: 0.39   best f1: 0.7947051105946379\n",
      "search finish.\n",
      "\n",
      "best auc: 0.9793638413713606\n",
      "best f1: 0.7947051105946379\n",
      "validate mean: 0.10758900272832304\n",
      "runtime: 26286.044641017914\n"
     ]
    }
   ],
   "source": [
    "print('=============================================== threshold search ===============================================')\n",
    "# f1阈值敏感，所以对阈值做一个简单的迭代搜索。\n",
    "t0 = 0.05\n",
    "v = 0.002\n",
    "best_t = t0\n",
    "best_f1 = 0\n",
    "for step in range(201):\n",
    "    curr_t = t0 + step * v\n",
    "    y = [1 if x >= curr_t else 0 for x in val_pred]\n",
    "    curr_f1 = f1_score(val_y, y)\n",
    "    if curr_f1 > best_f1:\n",
    "        best_t = curr_t\n",
    "        best_f1 = curr_f1\n",
    "        print('step: {}   best threshold: {}   best f1: {}'.format(step, best_t, best_f1))\n",
    "print('search finish.')\n",
    "\n",
    "val_pred = [1 if x >= best_t else 0 for x in val_pred]\n",
    "print('\\nbest auc:', best_auc)\n",
    "print('best f1:', f1_score(val_y, val_pred))\n",
    "print('validate mean:', np.mean(val_pred))\n",
    "print('runtime:', time.time() - t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============================================== sub save ===============================================\n",
      "runtime: 26305.168647289276\n",
      "finish.\n",
      "========================================================================================================\n"
     ]
    }
   ],
   "source": [
    "print('=============================================== sub save ===============================================')\n",
    "sub.to_csv('sub_prob_{}_{}_{}.csv'.format(best_auc, best_f1, sub['target'].mean()), index=False)\n",
    "sub['target'] = sub['target'].apply(lambda x: 1 if x >= best_t else 0)\n",
    "sub.to_csv('sub_{}_{}_{}.csv'.format(best_auc, best_f1, sub['target'].mean()), index=False)\n",
    "print('runtime:', time.time() - t)\n",
    "print('finish.')\n",
    "print('========================================================================================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
