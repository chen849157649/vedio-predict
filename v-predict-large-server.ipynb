{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total cpu count 8\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from lightgbm.sklearn import LGBMClassifier\n",
    "from sklearn.metrics import roc_auc_score, f1_score\n",
    "from scipy.stats import entropy\n",
    "from gensim.models import Word2Vec\n",
    "import time\n",
    "import gc\n",
    "import os\n",
    "\n",
    "import tqdm                                                                                                   \n",
    "import concurrent.futures\n",
    "import multiprocessing\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "num_processes = multiprocessing.cpu_count()\n",
    "print(\"total cpu count\", +num_processes) \n",
    "\n",
    "os.environ['NUMEXPR_MAX_THREADS'] = '10'\n",
    "\n",
    "from core.utils import timeit, reduce_mem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/media/ryan/F/deep-learning-data/turing/vedio-predict/\"\n",
    "\n",
    "path_sub = path + 'sub/'\n",
    "path_npy = path + 'npy/'\n",
    "path_data = path + 'raw/'\n",
    "path_model = path + 'model/'\n",
    "path_result = path + 'result/'\n",
    "path_pickle = path + 'pickle/'\n",
    "path_profile = path + 'profile/'\n",
    "\n",
    "debug_small = False\n",
    "\n",
    "if debug_small:\n",
    "    train_df = pd.read_pickle(path_pickle + 'train_small.pickle')\n",
    "    test_df = pd.read_pickle(path_pickle + 'test_small.pickle')\n",
    "    # app = pd.read_pickle(path_pickle + 'app_small.pickle')\n",
    "    # user = pd.read_pickle(path_pickle + 'user_small.pickle')\n",
    "else:\n",
    "    train_df = pd.read_pickle(path_pickle + 'train.pickle')\n",
    "    test_df = pd.read_pickle(path_pickle + 'test.pickle')\n",
    "    # app = pd.read_pickle(path_pickle + 'app.pickle')\n",
    "    # user = pd.read_pickle(path_pickle + 'user.pickle')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>deviceid</th>\n",
       "      <th>newsid</th>\n",
       "      <th>guid</th>\n",
       "      <th>pos</th>\n",
       "      <th>app_version</th>\n",
       "      <th>device_vendor</th>\n",
       "      <th>netmodel</th>\n",
       "      <th>osversion</th>\n",
       "      <th>lng</th>\n",
       "      <th>lat</th>\n",
       "      <th>device_version</th>\n",
       "      <th>ts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8b2d7f2aed47ab32e9c6ae4f5ae00147</td>\n",
       "      <td>8008333091915950969</td>\n",
       "      <td>9a2c909ebc47aec49d9c160cdb4a6572</td>\n",
       "      <td>1</td>\n",
       "      <td>2.1.5</td>\n",
       "      <td>HONOR</td>\n",
       "      <td>g4</td>\n",
       "      <td>9</td>\n",
       "      <td>1.125385e+02</td>\n",
       "      <td>3.783793e+01</td>\n",
       "      <td>STF-AL00</td>\n",
       "      <td>1573298086436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8b2d7f2aed47ab32e9c6ae4f5ae00147</td>\n",
       "      <td>8008333091915950969</td>\n",
       "      <td>9a2c909ebc47aec49d9c160cdb4a6572</td>\n",
       "      <td>1</td>\n",
       "      <td>2.1.5</td>\n",
       "      <td>HONOR</td>\n",
       "      <td>w</td>\n",
       "      <td>9</td>\n",
       "      <td>1.117312e+02</td>\n",
       "      <td>3.562274e+01</td>\n",
       "      <td>STF-AL00</td>\n",
       "      <td>1573298087570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>832aaa33cdf4a0938ba2c795eb3ffefd</td>\n",
       "      <td>4941885624885390992</td>\n",
       "      <td>d51a157d2b1e0e9aed4dd7f9900b85b2</td>\n",
       "      <td>2</td>\n",
       "      <td>1.9.9</td>\n",
       "      <td>vivo</td>\n",
       "      <td>w</td>\n",
       "      <td>8.1.0</td>\n",
       "      <td>4.940656e-324</td>\n",
       "      <td>4.940656e-324</td>\n",
       "      <td>V1818T</td>\n",
       "      <td>1573377075934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>832aaa33cdf4a0938ba2c795eb3ffefd</td>\n",
       "      <td>6088376349846612406</td>\n",
       "      <td>d51a157d2b1e0e9aed4dd7f9900b85b2</td>\n",
       "      <td>1</td>\n",
       "      <td>1.9.9</td>\n",
       "      <td>vivo</td>\n",
       "      <td>w</td>\n",
       "      <td>8.1.0</td>\n",
       "      <td>4.940656e-324</td>\n",
       "      <td>4.940656e-324</td>\n",
       "      <td>V1818T</td>\n",
       "      <td>1573377044359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>67dd9dac18cce1a6d79e8f20eefd98ab</td>\n",
       "      <td>5343094189765291622</td>\n",
       "      <td>625dc45744f59ddbc3ec8df161217188</td>\n",
       "      <td>0</td>\n",
       "      <td>2.1.1</td>\n",
       "      <td>xiaomi</td>\n",
       "      <td>w</td>\n",
       "      <td>9</td>\n",
       "      <td>1.167509e+02</td>\n",
       "      <td>3.656831e+01</td>\n",
       "      <td>Redmi Note 7</td>\n",
       "      <td>1573380989662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11376676</th>\n",
       "      <td>11376677</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>04d9da051fd19038824ff5e62231999b</td>\n",
       "      <td>2243097017724653319</td>\n",
       "      <td>22f0487299a136c32ca50881a9e7e837</td>\n",
       "      <td>0</td>\n",
       "      <td>2.1.5</td>\n",
       "      <td>HONOR</td>\n",
       "      <td>o</td>\n",
       "      <td>9</td>\n",
       "      <td>1.057023e+02</td>\n",
       "      <td>2.962280e+01</td>\n",
       "      <td>HRY-AL00a</td>\n",
       "      <td>1573208907130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11376677</th>\n",
       "      <td>11376678</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>04d9da051fd19038824ff5e62231999b</td>\n",
       "      <td>2263062300471437930</td>\n",
       "      <td>22f0487299a136c32ca50881a9e7e837</td>\n",
       "      <td>4</td>\n",
       "      <td>2.1.5</td>\n",
       "      <td>HONOR</td>\n",
       "      <td>o</td>\n",
       "      <td>9</td>\n",
       "      <td>1.057023e+02</td>\n",
       "      <td>2.962280e+01</td>\n",
       "      <td>HRY-AL00a</td>\n",
       "      <td>1573209232467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11376678</th>\n",
       "      <td>11376679</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>04d9da051fd19038824ff5e62231999b</td>\n",
       "      <td>3805019430149772895</td>\n",
       "      <td>22f0487299a136c32ca50881a9e7e837</td>\n",
       "      <td>0</td>\n",
       "      <td>2.1.5</td>\n",
       "      <td>HONOR</td>\n",
       "      <td>w</td>\n",
       "      <td>9</td>\n",
       "      <td>1.057023e+02</td>\n",
       "      <td>2.962280e+01</td>\n",
       "      <td>HRY-AL00a</td>\n",
       "      <td>1573253363853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11376679</th>\n",
       "      <td>11376680</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>04d9da051fd19038824ff5e62231999b</td>\n",
       "      <td>3834534693777959342</td>\n",
       "      <td>22f0487299a136c32ca50881a9e7e837</td>\n",
       "      <td>1</td>\n",
       "      <td>2.1.5</td>\n",
       "      <td>HONOR</td>\n",
       "      <td>o</td>\n",
       "      <td>9</td>\n",
       "      <td>1.057023e+02</td>\n",
       "      <td>2.962280e+01</td>\n",
       "      <td>HRY-AL00a</td>\n",
       "      <td>1573208896400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11376680</th>\n",
       "      <td>11376681</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>04d9da051fd19038824ff5e62231999b</td>\n",
       "      <td>7188998647991815858</td>\n",
       "      <td>22f0487299a136c32ca50881a9e7e837</td>\n",
       "      <td>0</td>\n",
       "      <td>2.1.5</td>\n",
       "      <td>HONOR</td>\n",
       "      <td>o</td>\n",
       "      <td>9</td>\n",
       "      <td>1.057022e+02</td>\n",
       "      <td>2.962216e+01</td>\n",
       "      <td>HRY-AL00a</td>\n",
       "      <td>1573253374968</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11376681 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                id  target  timestamp                          deviceid  \\\n",
       "0                1       0        NaN  8b2d7f2aed47ab32e9c6ae4f5ae00147   \n",
       "1                2       0        NaN  8b2d7f2aed47ab32e9c6ae4f5ae00147   \n",
       "2                3       0        NaN  832aaa33cdf4a0938ba2c795eb3ffefd   \n",
       "3                4       0        NaN  832aaa33cdf4a0938ba2c795eb3ffefd   \n",
       "4                5       0        NaN  67dd9dac18cce1a6d79e8f20eefd98ab   \n",
       "...            ...     ...        ...                               ...   \n",
       "11376676  11376677       0        NaN  04d9da051fd19038824ff5e62231999b   \n",
       "11376677  11376678       0        NaN  04d9da051fd19038824ff5e62231999b   \n",
       "11376678  11376679       0        NaN  04d9da051fd19038824ff5e62231999b   \n",
       "11376679  11376680       0        NaN  04d9da051fd19038824ff5e62231999b   \n",
       "11376680  11376681       0        NaN  04d9da051fd19038824ff5e62231999b   \n",
       "\n",
       "                       newsid                              guid  pos  \\\n",
       "0         8008333091915950969  9a2c909ebc47aec49d9c160cdb4a6572    1   \n",
       "1         8008333091915950969  9a2c909ebc47aec49d9c160cdb4a6572    1   \n",
       "2         4941885624885390992  d51a157d2b1e0e9aed4dd7f9900b85b2    2   \n",
       "3         6088376349846612406  d51a157d2b1e0e9aed4dd7f9900b85b2    1   \n",
       "4         5343094189765291622  625dc45744f59ddbc3ec8df161217188    0   \n",
       "...                       ...                               ...  ...   \n",
       "11376676  2243097017724653319  22f0487299a136c32ca50881a9e7e837    0   \n",
       "11376677  2263062300471437930  22f0487299a136c32ca50881a9e7e837    4   \n",
       "11376678  3805019430149772895  22f0487299a136c32ca50881a9e7e837    0   \n",
       "11376679  3834534693777959342  22f0487299a136c32ca50881a9e7e837    1   \n",
       "11376680  7188998647991815858  22f0487299a136c32ca50881a9e7e837    0   \n",
       "\n",
       "         app_version device_vendor netmodel osversion            lng  \\\n",
       "0              2.1.5         HONOR       g4         9   1.125385e+02   \n",
       "1              2.1.5         HONOR        w         9   1.117312e+02   \n",
       "2              1.9.9          vivo        w     8.1.0  4.940656e-324   \n",
       "3              1.9.9          vivo        w     8.1.0  4.940656e-324   \n",
       "4              2.1.1        xiaomi        w         9   1.167509e+02   \n",
       "...              ...           ...      ...       ...            ...   \n",
       "11376676       2.1.5         HONOR        o         9   1.057023e+02   \n",
       "11376677       2.1.5         HONOR        o         9   1.057023e+02   \n",
       "11376678       2.1.5         HONOR        w         9   1.057023e+02   \n",
       "11376679       2.1.5         HONOR        o         9   1.057023e+02   \n",
       "11376680       2.1.5         HONOR        o         9   1.057022e+02   \n",
       "\n",
       "                    lat device_version             ts  \n",
       "0          3.783793e+01       STF-AL00  1573298086436  \n",
       "1          3.562274e+01       STF-AL00  1573298087570  \n",
       "2         4.940656e-324         V1818T  1573377075934  \n",
       "3         4.940656e-324         V1818T  1573377044359  \n",
       "4          3.656831e+01   Redmi Note 7  1573380989662  \n",
       "...                 ...            ...            ...  \n",
       "11376676   2.962280e+01      HRY-AL00a  1573208907130  \n",
       "11376677   2.962280e+01      HRY-AL00a  1573209232467  \n",
       "11376678   2.962280e+01      HRY-AL00a  1573253363853  \n",
       "11376679   2.962280e+01      HRY-AL00a  1573208896400  \n",
       "11376680   2.962216e+01      HRY-AL00a  1573253374968  \n",
       "\n",
       "[11376681 rows x 15 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df = train_df[train_df.deviceid.str[-1] == '1']\n",
    "# test_df = test_df[test_df.deviceid.str[-1] == '1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============================================== read train ===============================================\n",
      "runtime: 21.164100646972656\n"
     ]
    }
   ],
   "source": [
    "print('=============================================== read train ===============================================')\n",
    "t = time.time()\n",
    "# train_df = pd.read_csv('dataset/train.csv')\n",
    "train_df['date'] = pd.to_datetime(\n",
    "    train_df['ts'].apply(lambda x: time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(x / 1000)))\n",
    ")\n",
    "train_df['day'] = train_df['date'].dt.day\n",
    "\n",
    "# 训练集中，day=7的个数为11个，day=8的为3,674,871。 day9，10也是解决40w\n",
    "# day=7占比不到1/百万，属于异常情况，去掉合理？ 线上的表现又会如何，为啥不是直接删除，这样有点过了\n",
    "# 这里为啥只是改了day，不去直接改ts和timestamp呢？\n",
    "train_df.loc[train_df['day'] == 7, 'day'] = 8\n",
    "train_df['hour'] = train_df['date'].dt.hour\n",
    "train_df['minute'] = train_df['date'].dt.minute\n",
    "train_num = train_df.shape[0]\n",
    "labels = train_df['target'].values\n",
    "print('runtime:', time.time() - t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============================================== click data ===============================================\n",
      "runtime: 27.9391667842865\n"
     ]
    }
   ],
   "source": [
    "print('=============================================== click data ===============================================')\n",
    "click_df = train_df[train_df['target'] == 1].sort_values('timestamp').reset_index(drop=True)\n",
    "click_df['exposure_click_gap'] = click_df['timestamp'] - click_df['ts']\n",
    "click_df = click_df[click_df['exposure_click_gap'] >= 0].reset_index(drop=True)\n",
    "click_df['date'] = pd.to_datetime(\n",
    "    click_df['timestamp'].apply(lambda x: time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(x / 1000)))\n",
    ")\n",
    "click_df['day'] = click_df['date'].dt.day\n",
    "# 同上对day==7的修改\n",
    "click_df.loc[click_df['day'] == 7, 'day'] = 8\n",
    "\n",
    "del train_df['target'], train_df['timestamp']\n",
    "\n",
    "# 这里为啥要把click_df的这些字段删除呢？\n",
    "for f in ['date', 'exposure_click_gap', 'timestamp', 'ts', 'target', 'hour', 'minute']:\n",
    "    del click_df[f]\n",
    "print('runtime:', time.time() - t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============================================== read test ===============================================\n",
      "runtime: 38.73506569862366\n"
     ]
    }
   ],
   "source": [
    "print('=============================================== read test ===============================================')\n",
    "test_df['date'] = pd.to_datetime(\n",
    "    test_df['ts'].apply(lambda x: time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(x / 1000)))\n",
    ")\n",
    "test_df['day'] = test_df['date'].dt.day\n",
    "\n",
    "# 测试集中，day=10的个数为32个，day=11的为3,653,560占比 1/十万，属于异常情况，去掉合理\n",
    "test_df.loc[test_df['day'] == 10, 'day'] = 11\n",
    "test_df['hour'] = test_df['date'].dt.hour\n",
    "test_df['minute'] = test_df['date'].dt.minute\n",
    "df = pd.concat([train_df, test_df], axis=0, ignore_index=False)\n",
    "del train_df, test_df, df['date']\n",
    "gc.collect()\n",
    "print('runtime:', time.time() - t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================= category encoding =============================================\n",
      "deviceid\n",
      "newsid\n",
      "pos\n",
      "app_version\n",
      "device_vendor\n",
      "netmodel\n",
      "osversion\n",
      "device_version\n",
      "lng\n",
      "lat\n",
      "lng_lat\n",
      "2580.12 Mb, 1390.40 Mb (46.11 %)\n",
      "77.91 Mb, 46.97 Mb (39.71 %)\n",
      "1204.05 Mb, 673.70 Mb (44.05 %)\n",
      "runtime: 169.73485732078552\n"
     ]
    }
   ],
   "source": [
    "print('============================================= category encoding =============================================')\n",
    "df['lng_lat'] = df['lng'].astype('str') + '_' + df['lat'].astype('str')\n",
    "del df['guid']\n",
    "click_df['lng_lat'] = click_df['lng'].astype('str') + '_' + click_df['lat'].astype('str')\n",
    "sort_df = df.sort_values('ts').reset_index(drop=True)\n",
    "cate_cols = [\n",
    "    'deviceid', 'newsid', 'pos', 'app_version', 'device_vendor',\n",
    "    'netmodel', 'osversion', 'device_version', 'lng', 'lat', 'lng_lat'\n",
    "]\n",
    "for f in cate_cols:\n",
    "    print(f)\n",
    "    map_dict = dict(zip(df[f].unique(), range(df[f].nunique())))\n",
    "    df[f] = df[f].map(map_dict).fillna(-1).astype('int32')\n",
    "    click_df[f] = click_df[f].map(map_dict).fillna(-1).astype('int32')\n",
    "    sort_df[f] = sort_df[f].map(map_dict).fillna(-1).astype('int32')\n",
    "    df[f + '_count'] = df[f].map(df[f].value_counts())\n",
    "df = reduce_mem(df)\n",
    "click_df = reduce_mem(click_df)\n",
    "sort_df = reduce_mem(sort_df)\n",
    "print('runtime:', time.time() - t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================= feat engineer =============================================\n",
      "*************************** history stats ***************************\n",
      "------------------ deviceid ------------------\n",
      "runtime: 181.16225504875183\n",
      "------------------ pos_deviceid ------------------\n",
      "runtime: 195.30335974693298\n",
      "2078.43 Mb, 1562.40 Mb (24.83 %)\n"
     ]
    }
   ],
   "source": [
    "print('============================================= feat engineer =============================================')\n",
    "\n",
    "print('*************************** history stats ***************************')\n",
    "for f in [\n",
    "    ['deviceid'],\n",
    "    ['pos', 'deviceid'],\n",
    "    # ...\n",
    "]:\n",
    "    print('------------------ {} ------------------'.format('_'.join(f)))\n",
    "\n",
    "    # 对前一天的点击次数进行统计\n",
    "    tmp = click_df[f + ['day', 'id']].groupby(f + ['day'], as_index=False)['id'].agg(\n",
    "        {'_'.join(f) + '_prev_day_click_count': 'count'})\n",
    "    tmp['day'] += 1\n",
    "    df = df.merge(tmp, on=f + ['day'], how='left')\n",
    "    df['_'.join(f) + '_prev_day_click_count'] = df['_'.join(f) + '_prev_day_click_count'].fillna(0)\n",
    "    df.loc[df['day'] == 8, '_'.join(f) + '_prev_day_click_count'] = None\n",
    "\n",
    "    # 对前一天的曝光量进行统计\n",
    "    tmp = df[f + ['day', 'id']].groupby(f + ['day'], as_index=False)['id'].agg(\n",
    "        {'_'.join(f) + '_prev_day_count': 'count'})\n",
    "    tmp['day'] += 1\n",
    "    df = df.merge(tmp, on=f + ['day'], how='left')\n",
    "    df['_'.join(f) + '_prev_day_count'] = df['_'.join(f) + '_prev_day_count'].fillna(0)\n",
    "    df.loc[df['day'] == 8, '_'.join(f) + '_prev_day_count'] = None\n",
    "\n",
    "    # 计算前一天的点击率\n",
    "    df['_'.join(f) + '_prev_day_ctr'] = df['_'.join(f) + '_prev_day_click_count'] / (\n",
    "            df['_'.join(f) + '_prev_day_count'] + df['_'.join(f) + '_prev_day_count'].mean())\n",
    "\n",
    "    del tmp\n",
    "    print('runtime:', time.time() - t)\n",
    "del click_df\n",
    "df = reduce_mem(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************************** exposure_ts_gap ***************************\n",
      "------------------ deviceid ------------------\n",
      "2709.12 Mb, 2135.76 Mb (21.16 %)\n",
      "runtime: 314.36668968200684\n",
      "------------------ newsid ------------------\n",
      "3282.48 Mb, 2709.12 Mb (17.47 %)\n",
      "runtime: 444.183531999588\n",
      "------------------ lng_lat ------------------\n",
      "3855.84 Mb, 3282.48 Mb (14.87 %)\n",
      "runtime: 566.2674934864044\n",
      "------------------ pos_deviceid ------------------\n",
      "4429.20 Mb, 3855.84 Mb (12.94 %)\n",
      "runtime: 698.9450771808624\n",
      "------------------ pos_newsid ------------------\n",
      "5002.56 Mb, 4429.20 Mb (11.46 %)\n",
      "runtime: 853.4127724170685\n",
      "------------------ pos_lng_lat ------------------\n",
      "5575.92 Mb, 5002.56 Mb (10.28 %)\n",
      "runtime: 999.0461049079895\n",
      "------------------ pos_deviceid_lng_lat ------------------\n",
      "6149.28 Mb, 5575.92 Mb (9.32 %)\n",
      "runtime: 1154.560673713684\n",
      "------------------ netmodel_deviceid ------------------\n",
      "6722.64 Mb, 6149.28 Mb (8.53 %)\n",
      "runtime: 1307.9333600997925\n",
      "------------------ pos_netmodel_deviceid ------------------\n",
      "7296.00 Mb, 6722.64 Mb (7.86 %)\n",
      "runtime: 1473.1176352500916\n",
      "------------------ netmodel_lng_lat ------------------\n",
      "7869.36 Mb, 7296.00 Mb (7.29 %)\n",
      "runtime: 1638.9999208450317\n",
      "------------------ deviceid_lng_lat ------------------\n",
      "8442.72 Mb, 7869.36 Mb (6.79 %)\n",
      "runtime: 1808.6834750175476\n",
      "------------------ netmodel_deviceid_lng_lat ------------------\n",
      "9016.08 Mb, 8442.72 Mb (6.36 %)\n",
      "runtime: 1988.890592098236\n",
      "------------------ pos_netmodel_lng_lat ------------------\n",
      "9589.44 Mb, 9016.08 Mb (5.98 %)\n",
      "runtime: 2172.421869277954\n",
      "------------------ pos_netmodel_deviceid_lng_lat ------------------\n",
      "10162.80 Mb, 9589.44 Mb (5.64 %)\n",
      "runtime: 2380.6680223941803\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('*************************** exposure_ts_gap ***************************')\n",
    "for f in [\n",
    "    ['deviceid'], ['newsid'], ['lng_lat'],\n",
    "    ['pos', 'deviceid'], ['pos', 'newsid'], ['pos', 'lng_lat'],\n",
    "    ['pos', 'deviceid', 'lng_lat'],\n",
    "    ['netmodel', 'deviceid'],\n",
    "    ['pos', 'netmodel', 'deviceid'],\n",
    "    ['netmodel', 'lng_lat'], ['deviceid', 'lng_lat'],\n",
    "    ['netmodel', 'deviceid', 'lng_lat'], ['pos', 'netmodel', 'lng_lat'],\n",
    "    ['pos', 'netmodel', 'deviceid', 'lng_lat']\n",
    "]:\n",
    "    print('------------------ {} ------------------'.format('_'.join(f)))\n",
    "\n",
    "    tmp = sort_df[f + ['ts']].groupby(f)\n",
    "    # 前x次、后x次曝光到当前的时间差\n",
    "    for gap in [1, 2, 3, 5, 10]:\n",
    "        sort_df['{}_prev{}_exposure_ts_gap'.format('_'.join(f), gap)] = tmp['ts'].shift(0) - tmp['ts'].shift(gap)\n",
    "        sort_df['{}_next{}_exposure_ts_gap'.format('_'.join(f), gap)] = tmp['ts'].shift(-gap) - tmp['ts'].shift(0)\n",
    "        tmp2 = sort_df[\n",
    "            f + ['ts', '{}_prev{}_exposure_ts_gap'.format('_'.join(f), gap),\n",
    "                 '{}_next{}_exposure_ts_gap'.format('_'.join(f), gap)]\n",
    "            ].drop_duplicates(f + ['ts']).reset_index(drop=True)\n",
    "        df = df.merge(tmp2, on=f + ['ts'], how='left')\n",
    "        del sort_df['{}_prev{}_exposure_ts_gap'.format('_'.join(f), gap)]\n",
    "        del sort_df['{}_next{}_exposure_ts_gap'.format('_'.join(f), gap)]\n",
    "        del tmp2\n",
    "\n",
    "    del tmp\n",
    "    df = reduce_mem(df)\n",
    "    print('runtime:', time.time() - t)\n",
    "del df['ts']\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success build sample.pickle\n"
     ]
    }
   ],
   "source": [
    "df.to_pickle(path_pickle + \"df_081_without_cross.pickle\")\n",
    "print(\"success build df_081_without_cross.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 重新加载数据处理\n",
    "df = pd.read_pickle(path_pickle + 'df_081_without_cross.pickle')\n",
    "cate_cols = [\n",
    "    'deviceid', 'newsid', 'pos', 'app_version', 'device_vendor',\n",
    "    'netmodel', 'osversion', 'device_version', 'lng', 'lat', 'lng_lat'\n",
    "]\n",
    "\n",
    "del df['id']\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql import Row\n",
    "# from pyspark import SparkConf\n",
    "# from pyspark import SparkContext\n",
    "\n",
    "# conf = SparkConf()\n",
    "# conf.setAppName(\"[陈亮时/149675]-[tf_format_test]\")\n",
    "# sc = SparkContext(conf=conf)\n",
    "    \n",
    "# l = [('Ankit',25),('Jalfaizy',22),('saurabh',20),('Bala',26)]\n",
    "# rdd = sc.parallelize(l)\n",
    "# people = rdd.map(lambda x: Row(name=x[0], age=int(x[1])))\n",
    "# schemaPeople = sqlContext.createDataFrame(people)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('*************************** cross feat (second order) ***************************')\n",
    "# # 二阶交叉特征，可以继续做更高阶的交叉。\n",
    "# def build_cross_feat(df, f, col):\n",
    "#     print('------------------ {} {} ------------------'.format(f, col))\n",
    "#     df = df.merge(df[[f, col]].groupby(f, as_index=False)[col].agg({\n",
    "#         'cross_{}_{}_nunique'.format(f, col): 'nunique',\n",
    "#         'cross_{}_{}_ent'.format(f, col): lambda x: entropy(x.value_counts() / x.shape[0])  # 熵\n",
    "#     }), on=f, how='left')\n",
    "#     if 'cross_{}_{}_count'.format(f, col) not in df.columns.values and 'cross_{}_{}_count'.format(col,\n",
    "#                                                                                                   f) not in df.columns.values:\n",
    "#         df = df.merge(df[[f, col, 'id']].groupby([f, col], as_index=False)['id'].agg({\n",
    "#             'cross_{}_{}_count'.format(f, col): 'count'  # 共现次数\n",
    "#         }), on=[f, col], how='left')\n",
    "#     if 'cross_{}_{}_count_ratio'.format(col, f) not in df.columns.values:\n",
    "#         df['cross_{}_{}_count_ratio'.format(col, f)] = df['cross_{}_{}_count'.format(f, col)] / df[\n",
    "#             f + '_count']  # 比例偏好\n",
    "#     if 'cross_{}_{}_count_ratio'.format(f, col) not in df.columns.values:\n",
    "#         df['cross_{}_{}_count_ratio'.format(f, col)] = df['cross_{}_{}_count'.format(f, col)] / df[\n",
    "#             col + '_count']  # 比例偏好\n",
    "#     df['cross_{}_{}_nunique_ratio_{}_count'.format(f, col, f)] = df['cross_{}_{}_nunique'.format(f, col)] / df[\n",
    "#         f + '_count']\n",
    "#     print('runtime:', time.time() - t)\n",
    "#     df = reduce_mem(df)\n",
    "#     return df\n",
    "        \n",
    "# cross_cols = ['deviceid', 'newsid', 'pos', 'netmodel', 'lng_lat']\n",
    "# f_col_tuple_list = []\n",
    "# for f in cross_cols:\n",
    "#     for col in cross_cols:\n",
    "#         if col == f:\n",
    "#             continue\n",
    "#         f_col_tuple_list.append((f, col))\n",
    "        \n",
    "# print(f_col_tuple_list)\n",
    "# # with concurrent.futures.ProcessPoolExecutor(num_processes) as pool:\n",
    "# #     df = list(tqdm.tqdm(pool.map(build_cross_feat, cross_cols, chunksize=10, total=df.shape[0])))\n",
    "# for tuple_o in tqdm.tqdm(f_col_tuple_list):\n",
    "#     print(tuple_o)\n",
    "#     df = build_cross_feat(df, tuple_o[0], tuple_o[1])\n",
    "\n",
    "# del df['id']\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************************** cross feat (second order) ***************************\n",
      "------------------ deviceid newsid ------------------\n",
      "runtime: 149.09444093704224\n",
      "------------------ deviceid pos ------------------\n",
      "runtime: 159.9373984336853\n",
      "------------------ deviceid netmodel ------------------\n",
      "runtime: 171.00094175338745\n",
      "------------------ deviceid lng_lat ------------------\n",
      "runtime: 182.43015956878662\n",
      "781.91 Mb, 645.73 Mb (17.42 %)\n",
      "------------------ newsid deviceid ------------------\n",
      "runtime: 575.3607921600342\n",
      "------------------ newsid pos ------------------\n",
      "runtime: 983.6665697097778\n",
      "------------------ newsid netmodel ------------------\n",
      "runtime: 1379.0395348072052\n",
      "------------------ newsid lng_lat ------------------\n",
      "runtime: 1771.5170404911041\n",
      "801.37 Mb, 682.78 Mb (14.80 %)\n",
      "------------------ pos deviceid ------------------\n",
      "runtime: 1775.6361346244812\n",
      "------------------ pos newsid ------------------\n",
      "runtime: 1778.2754430770874\n",
      "------------------ pos netmodel ------------------\n",
      "runtime: 1783.168639421463\n",
      "------------------ pos lng_lat ------------------\n",
      "runtime: 1788.4117560386658\n",
      "816.19 Mb, 720.77 Mb (11.69 %)\n",
      "------------------ netmodel deviceid ------------------\n",
      "runtime: 1794.0664312839508\n",
      "------------------ netmodel newsid ------------------\n",
      "runtime: 1797.8940091133118\n",
      "------------------ netmodel pos ------------------\n",
      "runtime: 1801.59801363945\n",
      "------------------ netmodel lng_lat ------------------\n",
      "runtime: 1808.9744846820831\n",
      "831.94 Mb, 751.34 Mb (9.69 %)\n",
      "------------------ lng_lat deviceid ------------------\n",
      "runtime: 1852.7943511009216\n",
      "------------------ lng_lat newsid ------------------\n",
      "runtime: 1894.8238821029663\n",
      "------------------ lng_lat pos ------------------\n",
      "runtime: 1936.789220571518\n",
      "------------------ lng_lat netmodel ------------------\n",
      "runtime: 1979.1730046272278\n",
      "840.28 Mb, 773.57 Mb (7.94 %)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('*************************** cross feat (second order) ***************************')\n",
    "# 二阶交叉特征，可以继续做更高阶的交叉。\n",
    "cross_cols = ['deviceid', 'newsid', 'pos', 'netmodel', 'lng_lat']\n",
    "for f in cross_cols:\n",
    "    for col in cross_cols:\n",
    "        if col == f:\n",
    "            continue\n",
    "        print('------------------ {} {} ------------------'.format(f, col))\n",
    "        df = df.merge(df[[f, col]].groupby(f, as_index=False)[col].agg({\n",
    "            'cross_{}_{}_nunique'.format(f, col): 'nunique',\n",
    "            'cross_{}_{}_ent'.format(f, col): lambda x: entropy(x.value_counts() / x.shape[0])  # 熵\n",
    "        }), on=f, how='left')\n",
    "        if 'cross_{}_{}_count'.format(f, col) not in df.columns.values and 'cross_{}_{}_count'.format(col,\n",
    "                                                                                                      f) not in df.columns.values:\n",
    "            df = df.merge(df[[f, col, 'id']].groupby([f, col], as_index=False)['id'].agg({\n",
    "                'cross_{}_{}_count'.format(f, col): 'count'  # 共现次数\n",
    "            }), on=[f, col], how='left')\n",
    "        if 'cross_{}_{}_count_ratio'.format(col, f) not in df.columns.values:\n",
    "            df['cross_{}_{}_count_ratio'.format(col, f)] = df['cross_{}_{}_count'.format(f, col)] / df[\n",
    "                f + '_count']  # 比例偏好\n",
    "        if 'cross_{}_{}_count_ratio'.format(f, col) not in df.columns.values:\n",
    "            df['cross_{}_{}_count_ratio'.format(f, col)] = df['cross_{}_{}_count'.format(f, col)] / df[\n",
    "                col + '_count']  # 比例偏好\n",
    "        df['cross_{}_{}_nunique_ratio_{}_count'.format(f, col, f)] = df['cross_{}_{}_nunique'.format(f, col)] / df[\n",
    "            f + '_count']\n",
    "        print('runtime:', time.time() - t)\n",
    "    df = reduce_mem(df)\n",
    "del df['id']\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************************** embedding ***************************\n",
      "====================================== deviceid newsid ======================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:MainThread:gensim.models.word2vec:collecting all words and their counts\n",
      "INFO:MainThread:gensim.models.word2vec:PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "INFO:MainThread:gensim.models.word2vec:collected 300717 word types from a corpus of 971439 raw words and 7049 sentences\n",
      "INFO:MainThread:gensim.models.word2vec:Loading a fresh vocabulary\n",
      "INFO:MainThread:gensim.models.word2vec:effective_min_count=5 retains 38716 unique words (12% of original 300717, drops 262001)\n",
      "INFO:MainThread:gensim.models.word2vec:effective_min_count=5 leaves 569569 word corpus (58% of original 971439, drops 401870)\n",
      "INFO:MainThread:gensim.models.word2vec:deleting the raw counts dictionary of 300717 items\n",
      "INFO:MainThread:gensim.models.word2vec:sample=0.001 downsamples 0 most-common words\n",
      "INFO:MainThread:gensim.models.word2vec:downsampling leaves estimated 569569 word corpus (100.0% of prior 569569)\n",
      "INFO:MainThread:gensim.models.word2vec:constructing a huffman tree from 38716 words\n",
      "INFO:MainThread:gensim.models.word2vec:built huffman tree with maximum node depth 17\n",
      "INFO:MainThread:gensim.models.base_any2vec:estimated required memory for 38716 words and 8 dimensions: 30817936 bytes\n",
      "INFO:MainThread:gensim.models.word2vec:resetting layer weights\n",
      "INFO:MainThread:gensim.models.base_any2vec:training model with 3 workers on 38716 vocabulary and 8 features, using sg=0 hs=1 sample=0.001 negative=5 window=5\n",
      "INFO:MainThread:gensim.models.base_any2vec:EPOCH 1 - PROGRESS: at 64.08% examples, 500836 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:MainThread:gensim.models.base_any2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:MainThread:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:MainThread:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:MainThread:gensim.models.base_any2vec:EPOCH - 1 : training on 971439 raw words (555389 effective words) took 1.1s, 513041 effective words/s\n",
      "INFO:MainThread:gensim.models.base_any2vec:EPOCH 2 - PROGRESS: at 50.53% examples, 390696 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:MainThread:gensim.models.base_any2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:MainThread:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:MainThread:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:MainThread:gensim.models.base_any2vec:EPOCH - 2 : training on 971439 raw words (555389 effective words) took 1.4s, 399671 effective words/s\n",
      "INFO:MainThread:gensim.models.base_any2vec:EPOCH 3 - PROGRESS: at 59.46% examples, 467498 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:MainThread:gensim.models.base_any2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:MainThread:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:MainThread:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:MainThread:gensim.models.base_any2vec:EPOCH - 3 : training on 971439 raw words (555389 effective words) took 1.1s, 483801 effective words/s\n",
      "INFO:MainThread:gensim.models.base_any2vec:EPOCH 4 - PROGRESS: at 65.65% examples, 510251 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:MainThread:gensim.models.base_any2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:MainThread:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:MainThread:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:MainThread:gensim.models.base_any2vec:EPOCH - 4 : training on 971439 raw words (555389 effective words) took 1.1s, 521422 effective words/s\n",
      "INFO:MainThread:gensim.models.base_any2vec:EPOCH 5 - PROGRESS: at 64.08% examples, 504906 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:MainThread:gensim.models.base_any2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:MainThread:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:MainThread:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:MainThread:gensim.models.base_any2vec:EPOCH - 5 : training on 971439 raw words (555389 effective words) took 1.1s, 515360 effective words/s\n",
      "INFO:MainThread:gensim.models.base_any2vec:training on a 4857195 raw words (2776945 effective words) took 5.8s, 480890 effective words/s\n",
      "/home/iflow/wanghuibo/apps/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:22: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "/home/iflow/wanghuibo/apps/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:23: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.54 Mb, 0.17 Mb (67.50 %)\n",
      "runtime: 2007.1937282085419\n",
      "====================================== newsid deviceid ======================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:MainThread:gensim.models.word2vec:collecting all words and their counts\n",
      "INFO:MainThread:gensim.models.word2vec:PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "INFO:MainThread:gensim.models.word2vec:PROGRESS: at sentence #10000, processed 210921 words, keeping 6641 word types\n",
      "INFO:MainThread:gensim.models.word2vec:PROGRESS: at sentence #20000, processed 299653 words, keeping 6846 word types\n",
      "INFO:MainThread:gensim.models.word2vec:PROGRESS: at sentence #30000, processed 367566 words, keeping 6925 word types\n",
      "INFO:MainThread:gensim.models.word2vec:PROGRESS: at sentence #40000, processed 422373 words, keeping 6965 word types\n",
      "INFO:MainThread:gensim.models.word2vec:PROGRESS: at sentence #50000, processed 469363 words, keeping 6980 word types\n",
      "INFO:MainThread:gensim.models.word2vec:PROGRESS: at sentence #60000, processed 509557 words, keeping 6990 word types\n",
      "INFO:MainThread:gensim.models.word2vec:PROGRESS: at sentence #70000, processed 544166 words, keeping 7002 word types\n",
      "INFO:MainThread:gensim.models.word2vec:PROGRESS: at sentence #80000, processed 578069 words, keeping 7009 word types\n",
      "INFO:MainThread:gensim.models.word2vec:PROGRESS: at sentence #90000, processed 606954 words, keeping 7022 word types\n",
      "INFO:MainThread:gensim.models.word2vec:PROGRESS: at sentence #100000, processed 635164 words, keeping 7027 word types\n",
      "INFO:MainThread:gensim.models.word2vec:PROGRESS: at sentence #110000, processed 660028 words, keeping 7029 word types\n",
      "INFO:MainThread:gensim.models.word2vec:PROGRESS: at sentence #120000, processed 683898 words, keeping 7032 word types\n",
      "INFO:MainThread:gensim.models.word2vec:PROGRESS: at sentence #130000, processed 705854 words, keeping 7034 word types\n",
      "INFO:MainThread:gensim.models.word2vec:PROGRESS: at sentence #140000, processed 725994 words, keeping 7037 word types\n",
      "INFO:MainThread:gensim.models.word2vec:PROGRESS: at sentence #150000, processed 747094 words, keeping 7040 word types\n",
      "INFO:MainThread:gensim.models.word2vec:PROGRESS: at sentence #160000, processed 765504 words, keeping 7041 word types\n",
      "INFO:MainThread:gensim.models.word2vec:PROGRESS: at sentence #170000, processed 783186 words, keeping 7041 word types\n",
      "INFO:MainThread:gensim.models.word2vec:PROGRESS: at sentence #180000, processed 800310 words, keeping 7041 word types\n",
      "INFO:MainThread:gensim.models.word2vec:PROGRESS: at sentence #190000, processed 816660 words, keeping 7044 word types\n",
      "INFO:MainThread:gensim.models.word2vec:PROGRESS: at sentence #200000, processed 832535 words, keeping 7045 word types\n",
      "INFO:MainThread:gensim.models.word2vec:PROGRESS: at sentence #210000, processed 847023 words, keeping 7046 word types\n",
      "INFO:MainThread:gensim.models.word2vec:PROGRESS: at sentence #220000, processed 861147 words, keeping 7046 word types\n",
      "INFO:MainThread:gensim.models.word2vec:PROGRESS: at sentence #230000, processed 874872 words, keeping 7046 word types\n",
      "INFO:MainThread:gensim.models.word2vec:PROGRESS: at sentence #240000, processed 888313 words, keeping 7047 word types\n",
      "INFO:MainThread:gensim.models.word2vec:PROGRESS: at sentence #250000, processed 906184 words, keeping 7047 word types\n",
      "INFO:MainThread:gensim.models.word2vec:PROGRESS: at sentence #260000, processed 922809 words, keeping 7049 word types\n",
      "INFO:MainThread:gensim.models.word2vec:PROGRESS: at sentence #270000, processed 936427 words, keeping 7049 word types\n",
      "INFO:MainThread:gensim.models.word2vec:PROGRESS: at sentence #280000, processed 948695 words, keeping 7049 word types\n",
      "INFO:MainThread:gensim.models.word2vec:PROGRESS: at sentence #290000, processed 960093 words, keeping 7049 word types\n",
      "INFO:MainThread:gensim.models.word2vec:PROGRESS: at sentence #300000, processed 970700 words, keeping 7049 word types\n",
      "INFO:MainThread:gensim.models.word2vec:collected 7049 word types from a corpus of 971439 raw words and 300717 sentences\n",
      "INFO:MainThread:gensim.models.word2vec:Loading a fresh vocabulary\n",
      "INFO:MainThread:gensim.models.word2vec:effective_min_count=5 retains 6623 unique words (93% of original 7049, drops 426)\n",
      "INFO:MainThread:gensim.models.word2vec:effective_min_count=5 leaves 969963 word corpus (99% of original 971439, drops 1476)\n",
      "INFO:MainThread:gensim.models.word2vec:deleting the raw counts dictionary of 7049 items\n",
      "INFO:MainThread:gensim.models.word2vec:sample=0.001 downsamples 50 most-common words\n",
      "INFO:MainThread:gensim.models.word2vec:downsampling leaves estimated 855232 word corpus (88.2% of prior 969963)\n",
      "INFO:MainThread:gensim.models.word2vec:constructing a huffman tree from 6623 words\n",
      "INFO:MainThread:gensim.models.word2vec:built huffman tree with maximum node depth 18\n",
      "INFO:MainThread:gensim.models.base_any2vec:estimated required memory for 6623 words and 8 dimensions: 5271908 bytes\n",
      "INFO:MainThread:gensim.models.word2vec:resetting layer weights\n",
      "INFO:MainThread:gensim.models.base_any2vec:training model with 3 workers on 6623 vocabulary and 8 features, using sg=0 hs=1 sample=0.001 negative=5 window=5\n",
      "INFO:MainThread:gensim.models.base_any2vec:EPOCH 1 - PROGRESS: at 37.61% examples, 591556 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:MainThread:gensim.models.base_any2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:MainThread:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:MainThread:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:MainThread:gensim.models.base_any2vec:EPOCH - 1 : training on 971439 raw words (855194 effective words) took 1.4s, 616347 effective words/s\n",
      "INFO:MainThread:gensim.models.base_any2vec:EPOCH 2 - PROGRESS: at 42.06% examples, 614967 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:MainThread:gensim.models.base_any2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:MainThread:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:MainThread:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:MainThread:gensim.models.base_any2vec:EPOCH - 2 : training on 971439 raw words (855050 effective words) took 1.3s, 638542 effective words/s\n",
      "INFO:MainThread:gensim.models.base_any2vec:EPOCH 3 - PROGRESS: at 43.51% examples, 624316 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:MainThread:gensim.models.base_any2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:MainThread:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:MainThread:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:MainThread:gensim.models.base_any2vec:EPOCH - 3 : training on 971439 raw words (854909 effective words) took 1.3s, 644204 effective words/s\n",
      "INFO:MainThread:gensim.models.base_any2vec:EPOCH 4 - PROGRESS: at 45.11% examples, 629565 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:MainThread:gensim.models.base_any2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:MainThread:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:MainThread:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:MainThread:gensim.models.base_any2vec:EPOCH - 4 : training on 971439 raw words (855657 effective words) took 1.3s, 647081 effective words/s\n",
      "INFO:MainThread:gensim.models.base_any2vec:EPOCH 5 - PROGRESS: at 43.51% examples, 624996 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:MainThread:gensim.models.base_any2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:MainThread:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:MainThread:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:MainThread:gensim.models.base_any2vec:EPOCH - 5 : training on 971439 raw words (855253 effective words) took 1.3s, 640004 effective words/s\n",
      "INFO:MainThread:gensim.models.base_any2vec:training on a 4857195 raw words (4276063 effective words) took 6.7s, 635526 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.94 Mb, 8.03 Mb (65.00 %)\n",
      "runtime: 2073.353637933731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 33%|███▎      | 1/3 [01:32<03:05, 92.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================== deviceid lng_lat ======================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:MainThread:gensim.models.word2vec:collecting all words and their counts\n",
      "INFO:MainThread:gensim.models.word2vec:PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "INFO:MainThread:gensim.models.word2vec:collected 28886 word types from a corpus of 971439 raw words and 7049 sentences\n",
      "INFO:MainThread:gensim.models.word2vec:Loading a fresh vocabulary\n",
      "INFO:MainThread:gensim.models.word2vec:effective_min_count=5 retains 20383 unique words (70% of original 28886, drops 8503)\n",
      "INFO:MainThread:gensim.models.word2vec:effective_min_count=5 leaves 946493 word corpus (97% of original 971439, drops 24946)\n",
      "INFO:MainThread:gensim.models.word2vec:deleting the raw counts dictionary of 28886 items\n",
      "INFO:MainThread:gensim.models.word2vec:sample=0.001 downsamples 8 most-common words\n",
      "INFO:MainThread:gensim.models.word2vec:downsampling leaves estimated 795349 word corpus (84.0% of prior 946493)\n",
      "INFO:MainThread:gensim.models.word2vec:constructing a huffman tree from 20383 words\n",
      "INFO:MainThread:gensim.models.word2vec:built huffman tree with maximum node depth 18\n",
      "INFO:MainThread:gensim.models.base_any2vec:estimated required memory for 20383 words and 8 dimensions: 16224868 bytes\n",
      "INFO:MainThread:gensim.models.word2vec:resetting layer weights\n",
      "INFO:MainThread:gensim.models.base_any2vec:training model with 3 workers on 20383 vocabulary and 8 features, using sg=0 hs=1 sample=0.001 negative=5 window=5\n",
      "INFO:MainThread:gensim.models.base_any2vec:EPOCH 1 - PROGRESS: at 90.99% examples, 762971 words/s, in_qsize 4, out_qsize 0\n",
      "INFO:MainThread:gensim.models.base_any2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:MainThread:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:MainThread:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:MainThread:gensim.models.base_any2vec:EPOCH - 1 : training on 971439 raw words (789845 effective words) took 1.0s, 763797 effective words/s\n",
      "INFO:MainThread:gensim.models.base_any2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:MainThread:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:MainThread:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:MainThread:gensim.models.base_any2vec:EPOCH - 2 : training on 971439 raw words (789985 effective words) took 1.0s, 825003 effective words/s\n",
      "INFO:MainThread:gensim.models.base_any2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:MainThread:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:MainThread:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:MainThread:gensim.models.base_any2vec:EPOCH - 3 : training on 971439 raw words (790071 effective words) took 0.9s, 851367 effective words/s\n",
      "INFO:MainThread:gensim.models.base_any2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:MainThread:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:MainThread:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:MainThread:gensim.models.base_any2vec:EPOCH - 4 : training on 971439 raw words (789778 effective words) took 0.9s, 922787 effective words/s\n",
      "INFO:MainThread:gensim.models.base_any2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:MainThread:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:MainThread:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:MainThread:gensim.models.base_any2vec:EPOCH - 5 : training on 971439 raw words (789792 effective words) took 1.0s, 829826 effective words/s\n",
      "INFO:MainThread:gensim.models.base_any2vec:training on a 4857195 raw words (3949471 effective words) took 4.7s, 833132 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.54 Mb, 0.17 Mb (67.50 %)\n",
      "runtime: 2095.6993174552917\n",
      "====================================== lng_lat deviceid ======================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:MainThread:gensim.models.word2vec:collecting all words and their counts\n",
      "INFO:MainThread:gensim.models.word2vec:PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "INFO:MainThread:gensim.models.word2vec:PROGRESS: at sentence #10000, processed 709405 words, keeping 6304 word types\n",
      "INFO:MainThread:gensim.models.word2vec:PROGRESS: at sentence #20000, processed 790029 words, keeping 6868 word types\n",
      "INFO:MainThread:gensim.models.word2vec:collected 7049 word types from a corpus of 971439 raw words and 28886 sentences\n",
      "INFO:MainThread:gensim.models.word2vec:Loading a fresh vocabulary\n",
      "INFO:MainThread:gensim.models.word2vec:effective_min_count=5 retains 6623 unique words (93% of original 7049, drops 426)\n",
      "INFO:MainThread:gensim.models.word2vec:effective_min_count=5 leaves 969963 word corpus (99% of original 971439, drops 1476)\n",
      "INFO:MainThread:gensim.models.word2vec:deleting the raw counts dictionary of 7049 items\n",
      "INFO:MainThread:gensim.models.word2vec:sample=0.001 downsamples 50 most-common words\n",
      "INFO:MainThread:gensim.models.word2vec:downsampling leaves estimated 855232 word corpus (88.2% of prior 969963)\n",
      "INFO:MainThread:gensim.models.word2vec:constructing a huffman tree from 6623 words\n",
      "INFO:MainThread:gensim.models.word2vec:built huffman tree with maximum node depth 18\n",
      "INFO:MainThread:gensim.models.base_any2vec:estimated required memory for 6623 words and 8 dimensions: 5271908 bytes\n",
      "INFO:MainThread:gensim.models.word2vec:resetting layer weights\n",
      "INFO:MainThread:gensim.models.base_any2vec:training model with 3 workers on 6623 vocabulary and 8 features, using sg=0 hs=1 sample=0.001 negative=5 window=5\n",
      "INFO:MainThread:gensim.models.base_any2vec:EPOCH 1 - PROGRESS: at 89.39% examples, 677356 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:MainThread:gensim.models.base_any2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:MainThread:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:MainThread:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:MainThread:gensim.models.base_any2vec:EPOCH - 1 : training on 971439 raw words (737896 effective words) took 1.1s, 695508 effective words/s\n",
      "INFO:MainThread:gensim.models.base_any2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:MainThread:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:MainThread:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:MainThread:gensim.models.base_any2vec:EPOCH - 2 : training on 971439 raw words (738269 effective words) took 0.8s, 971513 effective words/s\n",
      "INFO:MainThread:gensim.models.base_any2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:MainThread:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:MainThread:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:MainThread:gensim.models.base_any2vec:EPOCH - 3 : training on 971439 raw words (738259 effective words) took 1.0s, 764435 effective words/s\n",
      "INFO:MainThread:gensim.models.base_any2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:MainThread:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:MainThread:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:MainThread:gensim.models.base_any2vec:EPOCH - 4 : training on 971439 raw words (738337 effective words) took 0.7s, 1036311 effective words/s\n",
      "INFO:MainThread:gensim.models.base_any2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:MainThread:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:MainThread:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:MainThread:gensim.models.base_any2vec:EPOCH - 5 : training on 971439 raw words (738079 effective words) took 0.7s, 1077461 effective words/s\n",
      "INFO:MainThread:gensim.models.base_any2vec:training on a 4857195 raw words (3690840 effective words) took 4.2s, 879430 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.20 Mb, 0.72 Mb (67.50 %)\n",
      "runtime: 2115.7690057754517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 67%|██████▋   | 2/3 [02:14<01:17, 77.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================== newsid lng_lat ======================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:MainThread:gensim.models.word2vec:collecting all words and their counts\n",
      "INFO:MainThread:gensim.models.word2vec:PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "INFO:MainThread:gensim.models.word2vec:PROGRESS: at sentence #10000, processed 210921 words, keeping 20236 word types\n",
      "INFO:MainThread:gensim.models.word2vec:PROGRESS: at sentence #20000, processed 299653 words, keeping 23315 word types\n",
      "INFO:MainThread:gensim.models.word2vec:PROGRESS: at sentence #30000, processed 367566 words, keeping 24742 word types\n",
      "INFO:MainThread:gensim.models.word2vec:PROGRESS: at sentence #40000, processed 422373 words, keeping 25623 word types\n",
      "INFO:MainThread:gensim.models.word2vec:PROGRESS: at sentence #50000, processed 469363 words, keeping 26208 word types\n",
      "INFO:MainThread:gensim.models.word2vec:PROGRESS: at sentence #60000, processed 509557 words, keeping 26621 word types\n",
      "INFO:MainThread:gensim.models.word2vec:PROGRESS: at sentence #70000, processed 544166 words, keeping 26916 word types\n",
      "INFO:MainThread:gensim.models.word2vec:PROGRESS: at sentence #80000, processed 578069 words, keeping 27160 word types\n",
      "INFO:MainThread:gensim.models.word2vec:PROGRESS: at sentence #90000, processed 606954 words, keeping 27364 word types\n",
      "INFO:MainThread:gensim.models.word2vec:PROGRESS: at sentence #100000, processed 635164 words, keeping 27526 word types\n",
      "INFO:MainThread:gensim.models.word2vec:PROGRESS: at sentence #110000, processed 660028 words, keeping 27673 word types\n",
      "INFO:MainThread:gensim.models.word2vec:PROGRESS: at sentence #120000, processed 683898 words, keeping 27800 word types\n",
      "INFO:MainThread:gensim.models.word2vec:PROGRESS: at sentence #130000, processed 705854 words, keeping 27899 word types\n",
      "INFO:MainThread:gensim.models.word2vec:PROGRESS: at sentence #140000, processed 725994 words, keeping 28007 word types\n",
      "INFO:MainThread:gensim.models.word2vec:PROGRESS: at sentence #150000, processed 747094 words, keeping 28108 word types\n",
      "INFO:MainThread:gensim.models.word2vec:PROGRESS: at sentence #160000, processed 765504 words, keeping 28179 word types\n",
      "INFO:MainThread:gensim.models.word2vec:PROGRESS: at sentence #170000, processed 783186 words, keeping 28232 word types\n",
      "INFO:MainThread:gensim.models.word2vec:PROGRESS: at sentence #180000, processed 800310 words, keeping 28277 word types\n",
      "INFO:MainThread:gensim.models.word2vec:PROGRESS: at sentence #190000, processed 816660 words, keeping 28340 word types\n",
      "INFO:MainThread:gensim.models.word2vec:PROGRESS: at sentence #200000, processed 832535 words, keeping 28387 word types\n",
      "INFO:MainThread:gensim.models.word2vec:PROGRESS: at sentence #210000, processed 847023 words, keeping 28424 word types\n",
      "INFO:MainThread:gensim.models.word2vec:PROGRESS: at sentence #220000, processed 861147 words, keeping 28454 word types\n",
      "INFO:MainThread:gensim.models.word2vec:PROGRESS: at sentence #230000, processed 874872 words, keeping 28507 word types\n",
      "INFO:MainThread:gensim.models.word2vec:PROGRESS: at sentence #240000, processed 888313 words, keeping 28557 word types\n",
      "INFO:MainThread:gensim.models.word2vec:PROGRESS: at sentence #250000, processed 906184 words, keeping 28623 word types\n",
      "INFO:MainThread:gensim.models.word2vec:PROGRESS: at sentence #260000, processed 922809 words, keeping 28742 word types\n",
      "INFO:MainThread:gensim.models.word2vec:PROGRESS: at sentence #270000, processed 936427 words, keeping 28805 word types\n",
      "INFO:MainThread:gensim.models.word2vec:PROGRESS: at sentence #280000, processed 948695 words, keeping 28836 word types\n",
      "INFO:MainThread:gensim.models.word2vec:PROGRESS: at sentence #290000, processed 960093 words, keeping 28865 word types\n",
      "INFO:MainThread:gensim.models.word2vec:PROGRESS: at sentence #300000, processed 970700 words, keeping 28885 word types\n",
      "INFO:MainThread:gensim.models.word2vec:collected 28886 word types from a corpus of 971439 raw words and 300717 sentences\n",
      "INFO:MainThread:gensim.models.word2vec:Loading a fresh vocabulary\n",
      "INFO:MainThread:gensim.models.word2vec:effective_min_count=5 retains 20383 unique words (70% of original 28886, drops 8503)\n",
      "INFO:MainThread:gensim.models.word2vec:effective_min_count=5 leaves 946493 word corpus (97% of original 971439, drops 24946)\n",
      "INFO:MainThread:gensim.models.word2vec:deleting the raw counts dictionary of 28886 items\n",
      "INFO:MainThread:gensim.models.word2vec:sample=0.001 downsamples 8 most-common words\n",
      "INFO:MainThread:gensim.models.word2vec:downsampling leaves estimated 795349 word corpus (84.0% of prior 946493)\n",
      "INFO:MainThread:gensim.models.word2vec:constructing a huffman tree from 20383 words\n",
      "INFO:MainThread:gensim.models.word2vec:built huffman tree with maximum node depth 18\n",
      "INFO:MainThread:gensim.models.base_any2vec:estimated required memory for 20383 words and 8 dimensions: 16224868 bytes\n",
      "INFO:MainThread:gensim.models.word2vec:resetting layer weights\n",
      "INFO:MainThread:gensim.models.base_any2vec:training model with 3 workers on 20383 vocabulary and 8 features, using sg=0 hs=1 sample=0.001 negative=5 window=5\n",
      "INFO:MainThread:gensim.models.base_any2vec:EPOCH 1 - PROGRESS: at 26.55% examples, 450503 words/s, in_qsize 4, out_qsize 1\n",
      "INFO:MainThread:gensim.models.base_any2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:MainThread:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:MainThread:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:MainThread:gensim.models.base_any2vec:EPOCH - 1 : training on 971439 raw words (795483 effective words) took 1.7s, 464475 effective words/s\n",
      "INFO:MainThread:gensim.models.base_any2vec:EPOCH 2 - PROGRESS: at 34.93% examples, 522488 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:MainThread:gensim.models.base_any2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:MainThread:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:MainThread:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:MainThread:gensim.models.base_any2vec:EPOCH - 2 : training on 971439 raw words (795506 effective words) took 1.6s, 507818 effective words/s\n",
      "INFO:MainThread:gensim.models.base_any2vec:EPOCH 3 - PROGRESS: at 32.33% examples, 500767 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:MainThread:gensim.models.base_any2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:MainThread:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:MainThread:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:MainThread:gensim.models.base_any2vec:EPOCH - 3 : training on 971439 raw words (795354 effective words) took 1.6s, 496996 effective words/s\n",
      "INFO:MainThread:gensim.models.base_any2vec:EPOCH 4 - PROGRESS: at 40.51% examples, 559078 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:MainThread:gensim.models.base_any2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:MainThread:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:MainThread:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:MainThread:gensim.models.base_any2vec:EPOCH - 4 : training on 971439 raw words (795297 effective words) took 1.4s, 557617 effective words/s\n",
      "INFO:MainThread:gensim.models.base_any2vec:EPOCH 5 - PROGRESS: at 40.56% examples, 559296 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:MainThread:gensim.models.base_any2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:MainThread:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:MainThread:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:MainThread:gensim.models.base_any2vec:EPOCH - 5 : training on 971439 raw words (795430 effective words) took 1.5s, 545490 effective words/s\n",
      "INFO:MainThread:gensim.models.base_any2vec:training on a 4857195 raw words (3977070 effective words) took 7.8s, 511204 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.94 Mb, 8.03 Mb (65.00 %)\n",
      "runtime: 2184.097326517105\n",
      "====================================== lng_lat newsid ======================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:MainThread:gensim.models.word2vec:collecting all words and their counts\n",
      "INFO:MainThread:gensim.models.word2vec:PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "INFO:MainThread:gensim.models.word2vec:PROGRESS: at sentence #10000, processed 709405 words, keeping 249870 word types\n",
      "INFO:MainThread:gensim.models.word2vec:PROGRESS: at sentence #20000, processed 790029 words, keeping 262967 word types\n",
      "INFO:MainThread:gensim.models.word2vec:collected 300717 word types from a corpus of 971439 raw words and 28886 sentences\n",
      "INFO:MainThread:gensim.models.word2vec:Loading a fresh vocabulary\n",
      "INFO:MainThread:gensim.models.word2vec:effective_min_count=5 retains 38716 unique words (12% of original 300717, drops 262001)\n",
      "INFO:MainThread:gensim.models.word2vec:effective_min_count=5 leaves 569569 word corpus (58% of original 971439, drops 401870)\n",
      "INFO:MainThread:gensim.models.word2vec:deleting the raw counts dictionary of 300717 items\n",
      "INFO:MainThread:gensim.models.word2vec:sample=0.001 downsamples 0 most-common words\n",
      "INFO:MainThread:gensim.models.word2vec:downsampling leaves estimated 569569 word corpus (100.0% of prior 569569)\n",
      "INFO:MainThread:gensim.models.word2vec:constructing a huffman tree from 38716 words\n",
      "INFO:MainThread:gensim.models.word2vec:built huffman tree with maximum node depth 17\n",
      "INFO:MainThread:gensim.models.base_any2vec:estimated required memory for 38716 words and 8 dimensions: 30817936 bytes\n",
      "INFO:MainThread:gensim.models.word2vec:resetting layer weights\n",
      "INFO:MainThread:gensim.models.base_any2vec:training model with 3 workers on 38716 vocabulary and 8 features, using sg=0 hs=1 sample=0.001 negative=5 window=5\n",
      "INFO:MainThread:gensim.models.base_any2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:MainThread:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:MainThread:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:MainThread:gensim.models.base_any2vec:EPOCH - 1 : training on 971439 raw words (485179 effective words) took 1.0s, 495080 effective words/s\n",
      "INFO:MainThread:gensim.models.base_any2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:MainThread:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:MainThread:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:MainThread:gensim.models.base_any2vec:EPOCH - 2 : training on 971439 raw words (485179 effective words) took 1.0s, 503079 effective words/s\n",
      "INFO:MainThread:gensim.models.base_any2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:MainThread:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:MainThread:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:MainThread:gensim.models.base_any2vec:EPOCH - 3 : training on 971439 raw words (485179 effective words) took 1.0s, 495297 effective words/s\n",
      "INFO:MainThread:gensim.models.base_any2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:MainThread:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:MainThread:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:MainThread:gensim.models.base_any2vec:EPOCH - 4 : training on 971439 raw words (485179 effective words) took 1.0s, 488173 effective words/s\n",
      "INFO:MainThread:gensim.models.base_any2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:MainThread:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:MainThread:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:MainThread:gensim.models.base_any2vec:EPOCH - 5 : training on 971439 raw words (485179 effective words) took 0.9s, 532248 effective words/s\n",
      "INFO:MainThread:gensim.models.base_any2vec:training on a 4857195 raw words (2425895 effective words) took 4.8s, 500978 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.20 Mb, 0.72 Mb (67.50 %)\n",
      "runtime: 2213.092384815216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [03:52<00:00, 77.45s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('*************************** embedding ***************************')\n",
    "\n",
    "\n",
    "# 之前有个朋友给embedding做了一个我认为非常形象的比喻：\n",
    "# 在非诚勿扰上面，如果你想了解一个女嘉宾，那么你可以看看她都中意过哪些男嘉宾；\n",
    "# 反过来也一样，如果你想认识一个男嘉宾，那么你也可以看看他都选过哪些女嘉宾。\n",
    "\n",
    "\n",
    "def emb(df, f1, f2):\n",
    "    emb_size = 8\n",
    "    print('====================================== {} {} ======================================'.format(f1, f2))\n",
    "    tmp = df.groupby(f1, as_index=False)[f2].agg({'{}_{}_list'.format(f1, f2): list})\n",
    "    sentences = tmp['{}_{}_list'.format(f1, f2)].values.tolist()\n",
    "    del tmp['{}_{}_list'.format(f1, f2)]\n",
    "    for i in range(len(sentences)):\n",
    "        sentences[i] = [str(x) for x in sentences[i]]\n",
    "    model = Word2Vec(sentences, size=emb_size, window=5, min_count=5, sg=0, hs=1, seed=2019)\n",
    "    emb_matrix = []\n",
    "    for seq in sentences:\n",
    "        vec = []\n",
    "        for w in seq:\n",
    "            if w in model:\n",
    "                vec.append(model[w])\n",
    "        if len(vec) > 0:\n",
    "            emb_matrix.append(np.mean(vec, axis=0))\n",
    "        else:\n",
    "            emb_matrix.append([0] * emb_size)\n",
    "\n",
    "    # 为了支持数组多维处理，需要先做一个变换\n",
    "    emb_matrix = np.array(emb_matrix)\n",
    "\n",
    "    for i in range(emb_size):\n",
    "        tmp['{}_{}_emb_{}'.format(f1, f2, i)] = emb_matrix[:, i]\n",
    "    del model, emb_matrix, sentences\n",
    "    tmp = reduce_mem(tmp)\n",
    "    print('runtime:', time.time() - t)\n",
    "    return tmp\n",
    "\n",
    "\n",
    "emb_cols = [\n",
    "    ['deviceid', 'newsid'],\n",
    "    ['deviceid', 'lng_lat'],\n",
    "    ['newsid', 'lng_lat'],\n",
    "    # ...\n",
    "]\n",
    "for f1, f2 in tqdm.tqdm(emb_cols):\n",
    "    df = df.merge(emb(sort_df, f1, f2), on=f1, how='left')\n",
    "    df = df.merge(emb(sort_df, f2, f1), on=f2, how='left')\n",
    "del sort_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================== prepare train & valid  =============================================\n",
      "runtime: 63.83975410461426\n",
      "========================================================================================================\n"
     ]
    }
   ],
   "source": [
    "print('======================================== prepare train & valid  =============================================')\n",
    "train_df = df[:train_num].reset_index(drop=True)\n",
    "test_df = df[train_num:].reset_index(drop=True)\n",
    "del df\n",
    "gc.collect()\n",
    "\n",
    "train_idx = train_df[train_df['day'] < 10].index.tolist()\n",
    "val_idx = train_df[train_df['day'] == 10].index.tolist()\n",
    "\n",
    "train_x = train_df.iloc[train_idx].reset_index(drop=True)\n",
    "train_y = labels[train_idx]\n",
    "val_x = train_df.iloc[val_idx].reset_index(drop=True)\n",
    "val_y = labels[val_idx]\n",
    "\n",
    "del train_x['day'], val_x['day'], train_df['day'], test_df['day']\n",
    "gc.collect()\n",
    "print('runtime:', time.time() - t)\n",
    "print('========================================================================================================')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============================================== training validate ===============================================\n",
      "************** training **************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ryan/anaconda3/envs/tensorflow/lib/python3.6/site-packages/lightgbm/basic.py:1295: UserWarning: categorical_feature in Dataset is overridden.\n",
      "New categorical_feature is ['app_version', 'device_vendor', 'device_version', 'deviceid', 'lat', 'lng', 'lng_lat', 'netmodel', 'newsid', 'osversion', 'pos']\n",
      "  'New categorical_feature is {}'.format(sorted(list(categorical_feature))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds\n",
      "[50]\tvalid_0's auc: 0.965979\n",
      "[100]\tvalid_0's auc: 0.968006\n",
      "[150]\tvalid_0's auc: 0.969476\n",
      "[200]\tvalid_0's auc: 0.970556\n",
      "[250]\tvalid_0's auc: 0.97144\n",
      "[300]\tvalid_0's auc: 0.972456\n",
      "[350]\tvalid_0's auc: 0.973145\n",
      "[400]\tvalid_0's auc: 0.973808\n",
      "[450]\tvalid_0's auc: 0.974351\n",
      "[500]\tvalid_0's auc: 0.974893\n",
      "[550]\tvalid_0's auc: 0.975298\n",
      "[600]\tvalid_0's auc: 0.975613\n",
      "[650]\tvalid_0's auc: 0.975821\n",
      "[700]\tvalid_0's auc: 0.975969\n",
      "[750]\tvalid_0's auc: 0.976131\n",
      "[800]\tvalid_0's auc: 0.976241\n",
      "[850]\tvalid_0's auc: 0.976334\n",
      "[900]\tvalid_0's auc: 0.976423\n",
      "[950]\tvalid_0's auc: 0.976494\n",
      "[1000]\tvalid_0's auc: 0.976541\n",
      "[1050]\tvalid_0's auc: 0.976594\n",
      "[1100]\tvalid_0's auc: 0.976627\n",
      "[1150]\tvalid_0's auc: 0.97666\n",
      "[1200]\tvalid_0's auc: 0.976682\n",
      "[1250]\tvalid_0's auc: 0.976712\n",
      "[1300]\tvalid_0's auc: 0.976746\n",
      "[1350]\tvalid_0's auc: 0.97676\n",
      "[1400]\tvalid_0's auc: 0.976791\n",
      "[1450]\tvalid_0's auc: 0.976786\n",
      "[1500]\tvalid_0's auc: 0.976798\n",
      "[1550]\tvalid_0's auc: 0.976803\n",
      "[1600]\tvalid_0's auc: 0.976815\n",
      "[1650]\tvalid_0's auc: 0.976815\n",
      "[1700]\tvalid_0's auc: 0.976822\n",
      "[1750]\tvalid_0's auc: 0.976825\n",
      "[1800]\tvalid_0's auc: 0.976825\n",
      "[1850]\tvalid_0's auc: 0.976836\n",
      "[1900]\tvalid_0's auc: 0.976842\n",
      "[1950]\tvalid_0's auc: 0.97685\n",
      "[2000]\tvalid_0's auc: 0.976854\n",
      "[2050]\tvalid_0's auc: 0.976849\n",
      "[2100]\tvalid_0's auc: 0.976852\n",
      "[2150]\tvalid_0's auc: 0.976857\n",
      "[2200]\tvalid_0's auc: 0.976854\n",
      "[2250]\tvalid_0's auc: 0.97685\n",
      "[2300]\tvalid_0's auc: 0.976848\n",
      "[2350]\tvalid_0's auc: 0.976857\n",
      "Early stopping, best iteration is:\n",
      "[2175]\tvalid_0's auc: 0.97686\n",
      "runtime: 23766.210490226746\n"
     ]
    }
   ],
   "source": [
    "print('=============================================== training validate ===============================================')\n",
    "fea_imp_list = []\n",
    "clf = LGBMClassifier(\n",
    "    n_jobs=10,\n",
    "    learning_rate=0.01,\n",
    "    n_estimators=5000,\n",
    "    num_leaves=255,\n",
    "    subsample=0.9,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=2019,\n",
    "    metric=None\n",
    ")\n",
    "\n",
    "print('************** training **************')\n",
    "clf.fit(\n",
    "    train_x, train_y,\n",
    "    eval_set=[(val_x, val_y)],\n",
    "    eval_metric='auc',\n",
    "    categorical_feature=cate_cols,\n",
    "    early_stopping_rounds=200,\n",
    "    verbose=50\n",
    ")\n",
    "print('runtime:', time.time() - t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ryan/anaconda3/envs/tensorflow/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "### 再重新喂入新数据之前，试试save mode，看看提交的效果\n",
    "from sklearn.externals import joblib\n",
    "# joblib.dump(clf, path_model+'lgb.pkl')\n",
    "# load model\n",
    "clf = joblib.load(path_model + 'lgb.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************** training **************\n",
      "[50]\ttraining's binary_logloss: 0.21329\n",
      "[100]\ttraining's binary_logloss: 0.169338\n",
      "[150]\ttraining's binary_logloss: 0.146647\n",
      "[200]\ttraining's binary_logloss: 0.133552\n",
      "[250]\ttraining's binary_logloss: 0.125048\n",
      "[300]\ttraining's binary_logloss: 0.118899\n",
      "[350]\ttraining's binary_logloss: 0.114388\n",
      "[400]\ttraining's binary_logloss: 0.110993\n",
      "[450]\ttraining's binary_logloss: 0.108129\n",
      "[500]\ttraining's binary_logloss: 0.105638\n",
      "[550]\ttraining's binary_logloss: 0.103447\n",
      "[600]\ttraining's binary_logloss: 0.101452\n",
      "[650]\ttraining's binary_logloss: 0.0996984\n",
      "[700]\ttraining's binary_logloss: 0.0981395\n",
      "[750]\ttraining's binary_logloss: 0.0967756\n",
      "[800]\ttraining's binary_logloss: 0.0955186\n",
      "[850]\ttraining's binary_logloss: 0.0943431\n",
      "[900]\ttraining's binary_logloss: 0.0932685\n",
      "[950]\ttraining's binary_logloss: 0.0922471\n",
      "[1000]\ttraining's binary_logloss: 0.0912944\n",
      "[1050]\ttraining's binary_logloss: 0.0903772\n",
      "[1100]\ttraining's binary_logloss: 0.0894798\n",
      "[1150]\ttraining's binary_logloss: 0.0886183\n",
      "[1200]\ttraining's binary_logloss: 0.0878341\n",
      "[1250]\ttraining's binary_logloss: 0.0870575\n",
      "[1300]\ttraining's binary_logloss: 0.0863237\n",
      "[1350]\ttraining's binary_logloss: 0.0856295\n",
      "[1400]\ttraining's binary_logloss: 0.0849687\n",
      "[1450]\ttraining's binary_logloss: 0.0843123\n",
      "[1500]\ttraining's binary_logloss: 0.0836759\n",
      "[1550]\ttraining's binary_logloss: 0.083069\n",
      "[1600]\ttraining's binary_logloss: 0.0824826\n",
      "[1650]\ttraining's binary_logloss: 0.0819027\n",
      "[1700]\ttraining's binary_logloss: 0.0813576\n",
      "[1750]\ttraining's binary_logloss: 0.080826\n",
      "[1800]\ttraining's binary_logloss: 0.0803077\n",
      "[1850]\ttraining's binary_logloss: 0.0797913\n",
      "[1900]\ttraining's binary_logloss: 0.0792732\n",
      "[1950]\ttraining's binary_logloss: 0.0787912\n",
      "[2000]\ttraining's binary_logloss: 0.0783075\n",
      "[2050]\ttraining's binary_logloss: 0.0778547\n",
      "[2100]\ttraining's binary_logloss: 0.0774029\n"
     ]
    }
   ],
   "source": [
    "print('************** training **************')\n",
    "clf.fit(\n",
    "    train_df, labels,\n",
    "    eval_set=[(train_df, labels)],\n",
    "    categorical_feature=cate_cols,\n",
    "    verbose=50\n",
    ")\n",
    "print('runtime:', time.time() - t)\n",
    "\n",
    "print('************** test predict **************')\n",
    "sub = pd.read_csv(path_data + 'sample.csv')\n",
    "sub['target'] = clf.predict_proba(test_df)[:, 1]\n",
    "clf.predict_proba(test_df)[:, 1]\n",
    "fea_imp_list.append(clf.feature_importances_)\n",
    "print('runtime:', time.time() - t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "182"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************** test predict **************\n",
      "runtime: 37304.49480199814\n"
     ]
    }
   ],
   "source": [
    "### 临时试试如该不对training内容，再fit一次的话，提交的效果如何\n",
    "print('************** test predict **************')\n",
    "sub = pd.read_csv(path_data + 'sample.csv')\n",
    "# sub['target'] = clf.predict_proba(test_df)[:, 1]\n",
    "clf.predict_proba(test_df)[:, 1]\n",
    "fea_imp_list.append(clf.feature_importances_)\n",
    "print('runtime:', time.time() - t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============================================== feat importances ===============================================\n",
      "deviceid = 183689.0\n",
      "newsid = 86695.0\n",
      "device_version = 61750.0\n",
      "lat = 31214.0\n",
      "lng = 27944.0\n",
      "lng_lat = 21499.0\n",
      "netmodel_deviceid_lng_lat_next1_exposure_ts_gap = 6421.0\n",
      "netmodel_deviceid_next1_exposure_ts_gap = 5754.0\n",
      "pos = 5648.0\n",
      "pos_count = 5025.0\n",
      "deviceid_next3_exposure_ts_gap = 4803.0\n",
      "netmodel_deviceid_lng_lat_next2_exposure_ts_gap = 3996.0\n",
      "netmodel_deviceid_next3_exposure_ts_gap = 3688.0\n",
      "pos_netmodel_deviceid_next1_exposure_ts_gap = 3574.0\n",
      "deviceid_lng_lat_next3_exposure_ts_gap = 3547.0\n",
      "pos_netmodel_deviceid_lng_lat_next1_exposure_ts_gap = 3410.0\n",
      "netmodel_deviceid_next2_exposure_ts_gap = 3408.0\n",
      "deviceid_next1_exposure_ts_gap = 3391.0\n",
      "netmodel_deviceid_lng_lat_next3_exposure_ts_gap = 3093.0\n",
      "deviceid_next2_exposure_ts_gap = 2613.0\n",
      "netmodel_deviceid_next10_exposure_ts_gap = 2387.0\n",
      "deviceid_count = 2259.0\n",
      "deviceid_lng_lat_next1_exposure_ts_gap = 2041.0\n",
      "deviceid_next5_exposure_ts_gap = 2005.0\n",
      "pos_deviceid_prev_day_ctr = 1985.0\n",
      "netmodel_deviceid_next5_exposure_ts_gap = 1954.0\n",
      "netmodel_deviceid_lng_lat_next10_exposure_ts_gap = 1817.0\n",
      "deviceid_lng_lat_next2_exposure_ts_gap = 1728.0\n",
      "netmodel_count = 1725.0\n",
      "pos_netmodel_deviceid_next2_exposure_ts_gap = 1649.0\n",
      "pos_deviceid_next1_exposure_ts_gap = 1610.0\n",
      "pos_newsid_next1_exposure_ts_gap = 1607.0\n",
      "newsid_count = 1579.0\n",
      "pos_deviceid_lng_lat_next1_exposure_ts_gap = 1497.0\n",
      "newsid_next10_exposure_ts_gap = 1469.0\n",
      "pos_netmodel_deviceid_lng_lat_next2_exposure_ts_gap = 1373.0\n",
      "deviceid_lng_lat_next5_exposure_ts_gap = 1322.0\n",
      "netmodel_deviceid_lng_lat_next5_exposure_ts_gap = 1235.0\n",
      "deviceid_next10_exposure_ts_gap = 1177.0\n",
      "pos_netmodel_deviceid_next3_exposure_ts_gap = 1161.0\n",
      "deviceid_prev_day_ctr = 1128.0\n",
      "netmodel_deviceid_prev5_exposure_ts_gap = 1041.0\n",
      "deviceid_prev_day_count = 1025.0\n",
      "netmodel_deviceid_prev1_exposure_ts_gap = 949.0\n",
      "pos_newsid_prev1_exposure_ts_gap = 922.0\n",
      "newsid_next1_exposure_ts_gap = 881.0\n",
      "pos_netmodel_deviceid_prev1_exposure_ts_gap = 868.0\n",
      "netmodel_deviceid_prev10_exposure_ts_gap = 829.0\n",
      "pos_deviceid_lng_lat_next2_exposure_ts_gap = 788.0\n",
      "pos_deviceid_next2_exposure_ts_gap = 784.0\n",
      "netmodel_lng_lat_next1_exposure_ts_gap = 763.0\n",
      "pos_netmodel_deviceid_next5_exposure_ts_gap = 735.0\n",
      "newsid_prev10_exposure_ts_gap = 713.0\n",
      "pos_netmodel_deviceid_lng_lat_next3_exposure_ts_gap = 699.0\n",
      "pos_deviceid_prev1_exposure_ts_gap = 691.0\n",
      "deviceid_prev1_exposure_ts_gap = 689.0\n",
      "newsid_prev1_exposure_ts_gap = 689.0\n",
      "pos_newsid_next10_exposure_ts_gap = 675.0\n",
      "deviceid_lng_lat_next10_exposure_ts_gap = 656.0\n",
      "pos_deviceid_next3_exposure_ts_gap = 652.0\n",
      "newsid_next5_exposure_ts_gap = 648.0\n",
      "pos_netmodel_lng_lat_next1_exposure_ts_gap = 633.0\n",
      "pos_deviceid_prev_day_count = 621.0\n",
      "deviceid_prev2_exposure_ts_gap = 619.0\n",
      "netmodel_deviceid_prev3_exposure_ts_gap = 614.0\n",
      "netmodel_deviceid_lng_lat_prev1_exposure_ts_gap = 602.0\n",
      "netmodel = 586.0\n",
      "netmodel_deviceid_lng_lat_prev2_exposure_ts_gap = 568.0\n",
      "netmodel_deviceid_prev2_exposure_ts_gap = 567.0\n",
      "newsid_prev5_exposure_ts_gap = 546.0\n",
      "pos_netmodel_deviceid_next10_exposure_ts_gap = 513.0\n",
      "deviceid_prev10_exposure_ts_gap = 512.0\n",
      "pos_newsid_next5_exposure_ts_gap = 488.0\n",
      "pos_deviceid_next10_exposure_ts_gap = 474.0\n",
      "pos_newsid_next3_exposure_ts_gap = 462.0\n",
      "deviceid_prev5_exposure_ts_gap = 437.0\n",
      "pos_netmodel_deviceid_prev2_exposure_ts_gap = 434.0\n",
      "netmodel_lng_lat_next3_exposure_ts_gap = 433.0\n",
      "lng_lat_next3_exposure_ts_gap = 428.0\n",
      "deviceid_lng_lat_prev2_exposure_ts_gap = 428.0\n",
      "pos_netmodel_deviceid_prev10_exposure_ts_gap = 423.0\n",
      "deviceid_lng_lat_prev1_exposure_ts_gap = 422.0\n",
      "newsid_prev3_exposure_ts_gap = 411.0\n",
      "newsid_next3_exposure_ts_gap = 409.0\n",
      "newsid_next2_exposure_ts_gap = 405.0\n",
      "pos_deviceid_next5_exposure_ts_gap = 405.0\n",
      "netmodel_deviceid_lng_lat_prev3_exposure_ts_gap = 404.0\n",
      "lng_lat_next1_exposure_ts_gap = 396.0\n",
      "netmodel_lng_lat_next2_exposure_ts_gap = 393.0\n",
      "lng_lat_count = 388.0\n",
      "pos_netmodel_deviceid_prev3_exposure_ts_gap = 383.0\n",
      "netmodel_lng_lat_next10_exposure_ts_gap = 383.0\n",
      "pos_netmodel_deviceid_prev5_exposure_ts_gap = 364.0\n",
      "pos_newsid_prev10_exposure_ts_gap = 361.0\n",
      "newsid_prev2_exposure_ts_gap = 357.0\n",
      "pos_deviceid_lng_lat_prev1_exposure_ts_gap = 352.0\n",
      "lng_count = 348.0\n",
      "pos_netmodel_deviceid_lng_lat_prev1_exposure_ts_gap = 344.0\n",
      "pos_deviceid_prev10_exposure_ts_gap = 337.0\n",
      "pos_newsid_next2_exposure_ts_gap = 333.0\n",
      "netmodel_lng_lat_next5_exposure_ts_gap = 327.0\n",
      "device_version_count = 323.0\n",
      "pos_lng_lat_next1_exposure_ts_gap = 306.0\n",
      "minute = 302.0\n",
      "pos_deviceid_prev2_exposure_ts_gap = 302.0\n",
      "deviceid_prev3_exposure_ts_gap = 301.0\n",
      "lng_lat_next5_exposure_ts_gap = 297.0\n",
      "lng_lat_next2_exposure_ts_gap = 294.0\n",
      "pos_netmodel_deviceid_lng_lat_prev2_exposure_ts_gap = 283.0\n",
      "pos_deviceid_prev3_exposure_ts_gap = 279.0\n",
      "deviceid_prev_day_click_count = 278.0\n",
      "pos_newsid_prev3_exposure_ts_gap = 276.0\n",
      "pos_netmodel_deviceid_lng_lat_next5_exposure_ts_gap = 276.0\n",
      "lat_count = 275.0\n",
      "pos_newsid_prev5_exposure_ts_gap = 272.0\n",
      "netmodel_deviceid_lng_lat_prev10_exposure_ts_gap = 263.0\n",
      "pos_deviceid_prev5_exposure_ts_gap = 257.0\n",
      "netmodel_deviceid_lng_lat_prev5_exposure_ts_gap = 254.0\n",
      "pos_newsid_prev2_exposure_ts_gap = 252.0\n",
      "pos_deviceid_lng_lat_next3_exposure_ts_gap = 245.0\n",
      "lng_lat_next10_exposure_ts_gap = 244.0\n",
      "pos_netmodel_lng_lat_next2_exposure_ts_gap = 228.0\n",
      "deviceid_lng_lat_prev5_exposure_ts_gap = 226.0\n",
      "deviceid_lng_lat_prev3_exposure_ts_gap = 220.0\n",
      "pos_deviceid_prev_day_click_count = 219.0\n",
      "hour = 218.0\n",
      "pos_lng_lat_next2_exposure_ts_gap = 212.0\n",
      "app_version = 207.0\n",
      "netmodel_lng_lat_prev2_exposure_ts_gap = 206.0\n",
      "pos_netmodel_deviceid_lng_lat_next10_exposure_ts_gap = 194.0\n",
      "device_vendor = 189.0\n",
      "pos_lng_lat_prev1_exposure_ts_gap = 187.0\n",
      "pos_deviceid_lng_lat_next5_exposure_ts_gap = 183.0\n",
      "pos_lng_lat_next3_exposure_ts_gap = 177.0\n",
      "pos_netmodel_deviceid_lng_lat_prev3_exposure_ts_gap = 173.0\n",
      "lng_lat_prev1_exposure_ts_gap = 171.0\n",
      "lng_lat_prev2_exposure_ts_gap = 171.0\n",
      "pos_netmodel_lng_lat_next10_exposure_ts_gap = 170.0\n",
      "netmodel_lng_lat_prev1_exposure_ts_gap = 169.0\n",
      "lng_lat_prev10_exposure_ts_gap = 162.0\n",
      "pos_netmodel_lng_lat_prev1_exposure_ts_gap = 155.0\n",
      "pos_netmodel_lng_lat_next3_exposure_ts_gap = 155.0\n",
      "pos_deviceid_lng_lat_next10_exposure_ts_gap = 153.0\n",
      "pos_lng_lat_next5_exposure_ts_gap = 152.0\n",
      "pos_netmodel_deviceid_lng_lat_prev10_exposure_ts_gap = 150.0\n",
      "deviceid_lng_lat_prev10_exposure_ts_gap = 149.0\n",
      "netmodel_lng_lat_prev3_exposure_ts_gap = 147.0\n",
      "pos_netmodel_lng_lat_next5_exposure_ts_gap = 141.0\n",
      "pos_deviceid_lng_lat_prev2_exposure_ts_gap = 140.0\n",
      "pos_deviceid_lng_lat_prev3_exposure_ts_gap = 139.0\n",
      "pos_lng_lat_next10_exposure_ts_gap = 137.0\n",
      "pos_deviceid_lng_lat_prev10_exposure_ts_gap = 133.0\n",
      "lng_lat_prev5_exposure_ts_gap = 131.0\n",
      "pos_lng_lat_prev10_exposure_ts_gap = 131.0\n",
      "lng_lat_prev3_exposure_ts_gap = 128.0\n",
      "netmodel_lng_lat_prev10_exposure_ts_gap = 126.0\n",
      "pos_netmodel_deviceid_lng_lat_prev5_exposure_ts_gap = 125.0\n",
      "pos_lng_lat_prev2_exposure_ts_gap = 122.0\n",
      "pos_lng_lat_prev3_exposure_ts_gap = 120.0\n",
      "pos_netmodel_lng_lat_prev2_exposure_ts_gap = 117.0\n",
      "pos_deviceid_lng_lat_prev5_exposure_ts_gap = 114.0\n",
      "pos_netmodel_lng_lat_prev10_exposure_ts_gap = 111.0\n",
      "netmodel_lng_lat_prev5_exposure_ts_gap = 110.0\n",
      "pos_netmodel_lng_lat_prev3_exposure_ts_gap = 110.0\n",
      "app_version_count = 108.0\n",
      "pos_lng_lat_prev5_exposure_ts_gap = 107.0\n",
      "pos_netmodel_lng_lat_prev5_exposure_ts_gap = 103.0\n",
      "device_vendor_count = 86.0\n",
      "osversion_count = 84.0\n",
      "osversion = 28.0\n"
     ]
    }
   ],
   "source": [
    "print('=============================================== feat importances ===============================================')\n",
    "# 特征重要性可以好好看看\n",
    "fea_imp_dict = dict(zip(train_df.columns.values, np.mean(fea_imp_list, axis=0)))\n",
    "fea_imp_item = sorted(fea_imp_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "for f, imp in fea_imp_item:\n",
    "    print('{} = {}'.format(f, imp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3958109,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3958109, 170)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3653592,)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3653592, 170)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3653592,)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============================================== threshold search ===============================================\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [3958109, 3653592]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-5be4169b3620>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mcurr_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt0\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mcurr_t\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mval_pred\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mcurr_f1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcurr_f1\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbest_f1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mbest_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurr_t\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mf1_score\u001b[0;34m(y_true, y_pred, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1093\u001b[0m                        \u001b[0mpos_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpos_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maverage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1094\u001b[0m                        \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1095\u001b[0;31m                        zero_division=zero_division)\n\u001b[0m\u001b[1;32m   1096\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1097\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mfbeta_score\u001b[0;34m(y_true, y_pred, beta, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1220\u001b[0m                                                  \u001b[0mwarn_for\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'f-score'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                                                  \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1222\u001b[0;31m                                                  zero_division=zero_division)\n\u001b[0m\u001b[1;32m   1223\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mprecision_recall_fscore_support\u001b[0;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1478\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"beta should be >=0 in the F-beta score\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1479\u001b[0m     labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n\u001b[0;32m-> 1480\u001b[0;31m                                     pos_label)\n\u001b[0m\u001b[1;32m   1481\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1482\u001b[0m     \u001b[0;31m# Calculate tp_sum, pred_sum, true_sum ###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_set_wise_labels\u001b[0;34m(y_true, y_pred, average, labels, pos_label)\u001b[0m\n\u001b[1;32m   1295\u001b[0m                          str(average_options))\n\u001b[1;32m   1296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1297\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1298\u001b[0m     \u001b[0mpresent_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munique_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1299\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0maverage\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'binary'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0marray\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindicator\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \"\"\"\n\u001b[0;32m---> 80\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m     \u001b[0mtype_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0mtype_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0;32m--> 212\u001b[0;31m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [3958109, 3653592]"
     ]
    }
   ],
   "source": [
    "print('=============================================== threshold search ===============================================')\n",
    "# f1阈值敏感，所以对阈值做一个简单的迭代搜索。\n",
    "t0 = 0.05\n",
    "v = 0.002\n",
    "best_t = t0\n",
    "best_f1 = 0\n",
    "for step in range(201):\n",
    "    curr_t = t0 + step * v\n",
    "    y = [1 if x >= curr_t else 0 for x in val_pred]\n",
    "    curr_f1 = f1_score(val_y, y)\n",
    "    if curr_f1 > best_f1:\n",
    "        best_t = curr_t\n",
    "        best_f1 = curr_f1\n",
    "        print('step: {}   best threshold: {}   best f1: {}'.format(step, best_t, best_f1))\n",
    "print('search finish.')\n",
    "\n",
    "val_pred = [1 if x >= best_t else 0 for x in val_pred]\n",
    "print('\\nbest auc:', best_auc)\n",
    "print('best f1:', f1_score(val_y, val_pred))\n",
    "print('validate mean:', np.mean(val_pred))\n",
    "print('runtime:', time.time() - t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('=============================================== sub save ===============================================')\n",
    "sub.to_csv('sub_prob_{}_{}_{}.csv'.format(best_auc, best_f1, sub['target'].mean()), index=False)\n",
    "sub['target'] = sub['target'].apply(lambda x: 1 if x >= best_t else 0)\n",
    "sub.to_csv('sub_{}_{}_{}.csv'.format(best_auc, best_f1, sub['target'].mean()), index=False)\n",
    "print('runtime:', time.time() - t)\n",
    "print('finish.')\n",
    "print('========================================================================================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
