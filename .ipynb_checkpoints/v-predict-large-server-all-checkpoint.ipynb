{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total cpu count 8\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from lightgbm.sklearn import LGBMClassifier\n",
    "from sklearn.metrics import roc_auc_score, f1_score\n",
    "from scipy.stats import entropy\n",
    "from gensim.models import Word2Vec\n",
    "import time\n",
    "import gc\n",
    "import os\n",
    "\n",
    "import tqdm                                                                                                   \n",
    "import concurrent.futures\n",
    "import multiprocessing\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "num_processes = multiprocessing.cpu_count()\n",
    "print(\"total cpu count\", +num_processes) \n",
    "\n",
    "os.environ['NUMEXPR_MAX_THREADS'] = '8'\n",
    "\n",
    "from core.utils import timeit, reduce_mem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/media/ryan/F/deep-learning-data/turing/vedio-predict/\"\n",
    "\n",
    "path_sub = path + 'sub/'\n",
    "path_npy = path + 'npy/'\n",
    "path_data = path + 'raw/'\n",
    "path_model = path + 'model/'\n",
    "path_result = path + 'result/'\n",
    "path_pickle = path + 'pickle/'\n",
    "path_profile = path + 'profile/'\n",
    "\n",
    "debug_small = False\n",
    "\n",
    "if debug_small:\n",
    "    train_df = pd.read_pickle(path_pickle + 'train_small.pickle')\n",
    "    test_df = pd.read_pickle(path_pickle + 'test_small.pickle')\n",
    "    sub = pd.read_csv(path_data + 'sample.csv')\n",
    "\n",
    "    # app = pd.read_pickle(path_pickle + 'app_small.pickle')\n",
    "    # user = pd.read_pickle(path_pickle + 'user_small.pickle')\n",
    "else:\n",
    "    train_df = pd.read_pickle(path_pickle + 'train.pickle')\n",
    "    test_df = pd.read_pickle(path_pickle + 'test.pickle')\n",
    "    sub = pd.read_csv(path_data + 'sample.csv')\n",
    "\n",
    "    # app = pd.read_pickle(path_pickle + 'app.pickle')\n",
    "    # user = pd.read_pickle(path_pickle + 'user.pickle')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df = train_df[train_df.deviceid.str[-1] == '1']\n",
    "# test_df = test_df[test_df.deviceid.str[-1] == '1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sub = sub[sub.id.isin(test_df.id) ]\n",
    "# sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============================================== read train ===============================================\n",
      "runtime: 23.693913221359253\n"
     ]
    }
   ],
   "source": [
    "print('=============================================== read train ===============================================')\n",
    "t = time.time()\n",
    "# train_df = pd.read_csv('dataset/train.csv')\n",
    "train_df['date'] = pd.to_datetime(\n",
    "    train_df['ts'].apply(lambda x: time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(x / 1000)))\n",
    ")\n",
    "train_df['day'] = train_df['date'].dt.day\n",
    "\n",
    "# 训练集中，day=7的个数为11个，day=8的为3,674,871。 day9，10也是解决40w\n",
    "# day=7占比不到1/百万，属于异常情况，去掉合理？ 线上的表现又会如何，为啥不是直接删除，这样有点过了\n",
    "# 这里为啥只是改了day，不去直接改ts和timestamp呢？\n",
    "train_df.loc[train_df['day'] == 7, 'day'] = 8\n",
    "train_df['hour'] = train_df['date'].dt.hour\n",
    "train_df['minute'] = train_df['date'].dt.minute\n",
    "train_num = train_df.shape[0]\n",
    "labels = train_df['target'].values\n",
    "print('runtime:', time.time() - t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('=============================================== click data ===============================================')\n",
    "click_df = train_df[train_df['target'] == 1].sort_values('timestamp').reset_index(drop=True)\n",
    "click_df['exposure_click_gap'] = click_df['timestamp'] - click_df['ts']\n",
    "click_df = click_df[click_df['exposure_click_gap'] >= 0].reset_index(drop=True)\n",
    "click_df['date'] = pd.to_datetime(\n",
    "    click_df['timestamp'].apply(lambda x: time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(x / 1000)))\n",
    ")\n",
    "click_df['day'] = click_df['date'].dt.day\n",
    "# 同上对day==7的修改\n",
    "click_df.loc[click_df['day'] == 7, 'day'] = 8\n",
    "\n",
    "del train_df['target'], train_df['timestamp']\n",
    "\n",
    "# 这里为啥要把click_df的这些字段删除呢？\n",
    "for f in ['date', 'exposure_click_gap', 'timestamp', 'ts', 'target', 'hour', 'minute']:\n",
    "    del click_df[f]\n",
    "print('runtime:', time.time() - t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('=============================================== read test ===============================================')\n",
    "test_df['date'] = pd.to_datetime(\n",
    "    test_df['ts'].apply(lambda x: time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(x / 1000)))\n",
    ")\n",
    "test_df['day'] = test_df['date'].dt.day\n",
    "\n",
    "# 测试集中，day=10的个数为32个，day=11的为3,653,560占比 1/十万，属于异常情况，去掉合理\n",
    "test_df.loc[test_df['day'] == 10, 'day'] = 11\n",
    "test_df['hour'] = test_df['date'].dt.hour\n",
    "test_df['minute'] = test_df['date'].dt.minute\n",
    "df = pd.concat([train_df, test_df], axis=0, ignore_index=False)\n",
    "del train_df, test_df, df['date']\n",
    "gc.collect()\n",
    "print('runtime:', time.time() - t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('============================================= category encoding =============================================')\n",
    "df['lng_lat'] = df['lng'].astype('str') + '_' + df['lat'].astype('str')\n",
    "del df['guid']\n",
    "click_df['lng_lat'] = click_df['lng'].astype('str') + '_' + click_df['lat'].astype('str')\n",
    "sort_df = df.sort_values('ts').reset_index(drop=True)\n",
    "cate_cols = [\n",
    "    'deviceid', 'newsid', 'pos', 'app_version', 'device_vendor',\n",
    "    'netmodel', 'osversion', 'device_version', 'lng', 'lat', 'lng_lat'\n",
    "]\n",
    "for f in cate_cols:\n",
    "    print(f)\n",
    "    map_dict = dict(zip(df[f].unique(), range(df[f].nunique())))\n",
    "    df[f] = df[f].map(map_dict).fillna(-1).astype('int32')\n",
    "    click_df[f] = click_df[f].map(map_dict).fillna(-1).astype('int32')\n",
    "    sort_df[f] = sort_df[f].map(map_dict).fillna(-1).astype('int32')\n",
    "    df[f + '_count'] = df[f].map(df[f].value_counts())\n",
    "df = reduce_mem(df)\n",
    "click_df = reduce_mem(click_df)\n",
    "sort_df = reduce_mem(sort_df)\n",
    "print('runtime:', time.time() - t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('============================================= feat engineer =============================================')\n",
    "\n",
    "print('*************************** history stats ***************************')\n",
    "for f in [\n",
    "    ['deviceid'],\n",
    "    ['pos', 'deviceid'],\n",
    "    # ...\n",
    "]:\n",
    "    print('------------------ {} ------------------'.format('_'.join(f)))\n",
    "\n",
    "    # 对前一天的点击次数进行统计\n",
    "    tmp = click_df[f + ['day', 'id']].groupby(f + ['day'], as_index=False)['id'].agg(\n",
    "        {'_'.join(f) + '_prev_day_click_count': 'count'})\n",
    "    tmp['day'] += 1\n",
    "    df = df.merge(tmp, on=f + ['day'], how='left')\n",
    "    df['_'.join(f) + '_prev_day_click_count'] = df['_'.join(f) + '_prev_day_click_count'].fillna(0)\n",
    "    df.loc[df['day'] == 8, '_'.join(f) + '_prev_day_click_count'] = None\n",
    "\n",
    "    # 对前一天的曝光量进行统计\n",
    "    tmp = df[f + ['day', 'id']].groupby(f + ['day'], as_index=False)['id'].agg(\n",
    "        {'_'.join(f) + '_prev_day_count': 'count'})\n",
    "    tmp['day'] += 1\n",
    "    df = df.merge(tmp, on=f + ['day'], how='left')\n",
    "    df['_'.join(f) + '_prev_day_count'] = df['_'.join(f) + '_prev_day_count'].fillna(0)\n",
    "    df.loc[df['day'] == 8, '_'.join(f) + '_prev_day_count'] = None\n",
    "\n",
    "    # 计算前一天的点击率\n",
    "    df['_'.join(f) + '_prev_day_ctr'] = df['_'.join(f) + '_prev_day_click_count'] / (\n",
    "            df['_'.join(f) + '_prev_day_count'] + df['_'.join(f) + '_prev_day_count'].mean())\n",
    "\n",
    "    del tmp\n",
    "    print('runtime:', time.time() - t)\n",
    "del click_df\n",
    "df = reduce_mem(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('*************************** exposure_ts_gap ***************************')\n",
    "for f in [\n",
    "    ['deviceid'], ['newsid'], ['lng_lat'],\n",
    "    ['pos', 'deviceid'], ['pos', 'newsid'], ['pos', 'lng_lat'],\n",
    "    ['pos', 'deviceid', 'lng_lat'],\n",
    "    ['netmodel', 'deviceid'],\n",
    "    ['pos', 'netmodel', 'deviceid'],\n",
    "    ['netmodel', 'lng_lat'], ['deviceid', 'lng_lat'],\n",
    "    ['netmodel', 'deviceid', 'lng_lat'], ['pos', 'netmodel', 'lng_lat'],\n",
    "    ['pos', 'netmodel', 'deviceid', 'lng_lat']\n",
    "]:\n",
    "    print('------------------ {} ------------------'.format('_'.join(f)))\n",
    "\n",
    "    tmp = sort_df[f + ['ts']].groupby(f)\n",
    "    # 前x次、后x次曝光到当前的时间差\n",
    "    for gap in [1, 2, 3, 5, 10]:\n",
    "        sort_df['{}_prev{}_exposure_ts_gap'.format('_'.join(f), gap)] = tmp['ts'].shift(0) - tmp['ts'].shift(gap)\n",
    "        sort_df['{}_next{}_exposure_ts_gap'.format('_'.join(f), gap)] = tmp['ts'].shift(-gap) - tmp['ts'].shift(0)\n",
    "        tmp2 = sort_df[\n",
    "            f + ['ts', '{}_prev{}_exposure_ts_gap'.format('_'.join(f), gap),\n",
    "                 '{}_next{}_exposure_ts_gap'.format('_'.join(f), gap)]\n",
    "            ].drop_duplicates(f + ['ts']).reset_index(drop=True)\n",
    "        df = df.merge(tmp2, on=f + ['ts'], how='left')\n",
    "        del sort_df['{}_prev{}_exposure_ts_gap'.format('_'.join(f), gap)]\n",
    "        del sort_df['{}_next{}_exposure_ts_gap'.format('_'.join(f), gap)]\n",
    "        del tmp2\n",
    "\n",
    "    del tmp\n",
    "    df = reduce_mem(df)\n",
    "    print('runtime:', time.time() - t)\n",
    "del df['ts']\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('*************************** cross feat (second order) ***************************')\n",
    "# # 二阶交叉特征，可以继续做更高阶的交叉。\n",
    "# def build_cross_feat(df, f, col):\n",
    "#     print('------------------ {} {} ------------------'.format(f, col))\n",
    "#     df = df.merge(df[[f, col]].groupby(f, as_index=False)[col].agg({\n",
    "#         'cross_{}_{}_nunique'.format(f, col): 'nunique',\n",
    "#         'cross_{}_{}_ent'.format(f, col): lambda x: entropy(x.value_counts() / x.shape[0])  # 熵\n",
    "#     }), on=f, how='left')\n",
    "#     if 'cross_{}_{}_count'.format(f, col) not in df.columns.values and 'cross_{}_{}_count'.format(col,\n",
    "#                                                                                                   f) not in df.columns.values:\n",
    "#         df = df.merge(df[[f, col, 'id']].groupby([f, col], as_index=False)['id'].agg({\n",
    "#             'cross_{}_{}_count'.format(f, col): 'count'  # 共现次数\n",
    "#         }), on=[f, col], how='left')\n",
    "#     if 'cross_{}_{}_count_ratio'.format(col, f) not in df.columns.values:\n",
    "#         df['cross_{}_{}_count_ratio'.format(col, f)] = df['cross_{}_{}_count'.format(f, col)] / df[\n",
    "#             f + '_count']  # 比例偏好\n",
    "#     if 'cross_{}_{}_count_ratio'.format(f, col) not in df.columns.values:\n",
    "#         df['cross_{}_{}_count_ratio'.format(f, col)] = df['cross_{}_{}_count'.format(f, col)] / df[\n",
    "#             col + '_count']  # 比例偏好\n",
    "#     df['cross_{}_{}_nunique_ratio_{}_count'.format(f, col, f)] = df['cross_{}_{}_nunique'.format(f, col)] / df[\n",
    "#         f + '_count']\n",
    "#     print('runtime:', time.time() - t)\n",
    "#     df = reduce_mem(df)\n",
    "#     return df\n",
    "        \n",
    "# cross_cols = ['deviceid', 'newsid', 'pos', 'netmodel', 'lng_lat']\n",
    "# f_col_tuple_list = []\n",
    "# for f in cross_cols:\n",
    "#     for col in cross_cols:\n",
    "#         if col == f:\n",
    "#             continue\n",
    "#         f_col_tuple_list.append((f, col))\n",
    "        \n",
    "# print(f_col_tuple_list)\n",
    "# # with concurrent.futures.ProcessPoolExecutor(num_processes) as pool:\n",
    "# #     df = list(tqdm.tqdm(pool.map(build_cross_feat, cross_cols, chunksize=10, total=df.shape[0])))\n",
    "# for tuple_o in tqdm.tqdm(f_col_tuple_list):\n",
    "#     print(tuple_o)\n",
    "#     df = build_cross_feat(df, tuple_o[0], tuple_o[1])\n",
    "\n",
    "# del df['id']\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('*************************** cross feat (second order) ***************************')\n",
    "# 二阶交叉特征，可以继续做更高阶的交叉。\n",
    "cross_cols = ['deviceid', 'newsid', 'pos', 'netmodel', 'lng_lat']\n",
    "for f in cross_cols:\n",
    "    for col in cross_cols:\n",
    "        if col == f:\n",
    "            continue\n",
    "        print('------------------ {} {} ------------------'.format(f, col))\n",
    "        if  'cross_{}_{}_nunique'.format(f, col) not in df.columns.values:\n",
    "            df = df.merge(df[[f, col]].groupby(f, as_index=False)[col].agg({\n",
    "                'cross_{}_{}_nunique'.format(f, col): 'nunique',\n",
    "                'cross_{}_{}_ent'.format(f, col): lambda x: entropy(x.value_counts() / x.shape[0])  # 熵\n",
    "            }), on=f, how='left')\n",
    "        if 'cross_{}_{}_count'.format(f, col) not in df.columns.values and 'cross_{}_{}_count'.format(col,\n",
    "                                                                                                      f) not in df.columns.values:\n",
    "            df = df.merge(df[[f, col, 'id']].groupby([f, col], as_index=False)['id'].agg({\n",
    "                'cross_{}_{}_count'.format(f, col): 'count'  # 共现次数\n",
    "            }), on=[f, col], how='left')\n",
    "        if 'cross_{}_{}_count_ratio'.format(col, f) not in df.columns.values:\n",
    "            df['cross_{}_{}_count_ratio'.format(col, f)] = df['cross_{}_{}_count'.format(f, col)] / df[\n",
    "                f + '_count']  # 比例偏好\n",
    "        if 'cross_{}_{}_count_ratio'.format(f, col) not in df.columns.values:\n",
    "            df['cross_{}_{}_count_ratio'.format(f, col)] = df['cross_{}_{}_count'.format(f, col)] / df[\n",
    "                col + '_count']  # 比例偏好\n",
    "        df['cross_{}_{}_nunique_ratio_{}_count'.format(f, col, f)] = df['cross_{}_{}_nunique'.format(f, col)] / df[\n",
    "            f + '_count']\n",
    "        print('runtime:', time.time() - t)\n",
    "    df = reduce_mem(df)\n",
    "del df['id']\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle(path_pickle + \"df_081_cross.pickle\")\n",
    "print(\"success build df_081_cross.pickle\")\n",
    "\n",
    "sort_df.to_pickle(path_pickle + \"sort_df_081_cross.pickle\")\n",
    "print(\"success build sort_df_081_cross.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df =pd.read_pickle(path_pickle + \"df_081_cross.pickle\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('*************************** embedding ***************************')\n",
    "\n",
    "\n",
    "# 之前有个朋友给embedding做了一个我认为非常形象的比喻：\n",
    "# 在非诚勿扰上面，如果你想了解一个女嘉宾，那么你可以看看她都中意过哪些男嘉宾；\n",
    "# 反过来也一样，如果你想认识一个男嘉宾，那么你也可以看看他都选过哪些女嘉宾。\n",
    "\n",
    "\n",
    "def emb(df, f1, f2):\n",
    "    emb_size = 8\n",
    "    print('====================================== {} {} ======================================'.format(f1, f2))\n",
    "    tmp = df.groupby(f1, as_index=False)[f2].agg({'{}_{}_list'.format(f1, f2): list})\n",
    "    sentences = tmp['{}_{}_list'.format(f1, f2)].values.tolist()\n",
    "    del tmp['{}_{}_list'.format(f1, f2)]\n",
    "    for i in range(len(sentences)):\n",
    "        sentences[i] = [str(x) for x in sentences[i]]\n",
    "    model = Word2Vec(sentences, size=emb_size, window=5, min_count=5, sg=0, hs=1, seed=2019)\n",
    "    emb_matrix = []\n",
    "    for seq in sentences:\n",
    "        vec = []\n",
    "        for w in seq:\n",
    "            if w in model:\n",
    "                vec.append(model[w])\n",
    "        if len(vec) > 0:\n",
    "            emb_matrix.append(np.mean(vec, axis=0))\n",
    "        else:\n",
    "            emb_matrix.append([0] * emb_size)\n",
    "\n",
    "    # 为了支持数组多维处理，需要先做一个变换\n",
    "    emb_matrix = np.array(emb_matrix)\n",
    "\n",
    "    for i in range(emb_size):\n",
    "        tmp['{}_{}_emb_{}'.format(f1, f2, i)] = emb_matrix[:, i]\n",
    "    del model, emb_matrix, sentences\n",
    "    tmp = reduce_mem(tmp)\n",
    "    print('runtime:', time.time() - t)\n",
    "    return tmp\n",
    "\n",
    "\n",
    "emb_cols = [\n",
    "    ['deviceid', 'newsid'],\n",
    "    ['deviceid', 'lng_lat'],\n",
    "    ['newsid', 'lng_lat'],\n",
    "    # ...\n",
    "]\n",
    "for f1, f2 in tqdm.tqdm(emb_cols):\n",
    "    df = df.merge(emb(sort_df, f1, f2), on=f1, how='left')\n",
    "    df = df.merge(emb(sort_df, f2, f1), on=f2, how='left')\n",
    "del sort_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle(path_pickle + \"df_081_emd_all.pickle\")\n",
    "print(\"success build df_081_all.pickle\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df =pd.read_pickle(path_pickle + \"df_081_emd_all.pickle\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_num =11376681\n",
    "cate_cols = [\n",
    "    'deviceid', 'newsid', 'pos', 'app_version', 'device_vendor',\n",
    "    'netmodel', 'osversion', 'device_version', 'lng', 'lat', 'lng_lat'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('======================================== prepare train & valid  =============================================')\n",
    "train_df = df[:train_num].reset_index(drop=True)\n",
    "test_df = df[train_num:].reset_index(drop=True)\n",
    "del df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11376681, 268)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'day'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2896\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2897\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2898\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'day'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-b169b6d4f621>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'day'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mval_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'day'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtrain_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtrain_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2993\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2994\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2995\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2996\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2997\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2897\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2898\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2899\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2900\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2901\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'day'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "train_idx = train_df[train_df['day'] < 10].index.tolist()\n",
    "val_idx = train_df[train_df['day'] == 10].index.tolist()\n",
    "\n",
    "train_x = train_df.iloc[train_idx].reset_index(drop=True)\n",
    "train_y = labels[train_idx]\n",
    "val_x = train_df.iloc[val_idx].reset_index(drop=True)\n",
    "val_y = labels[val_idx]\n",
    "\n",
    "del train_x['day'], val_x['day'], train_df['day'], test_df['day']\n",
    "gc.collect()\n",
    "print('runtime:', time.time() - t)\n",
    "print('========================================================================================================')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_rate_callback(env):\n",
    "    iteration = env.iteration\n",
    "    if iteration % 10 == 0:\n",
    "        learning_rate = env.params['learning_rate'] * 0.99\n",
    "        env.params['learning_rate'] = learning_rate\n",
    "        \n",
    "        print('---- current learning rate:' + str(learning_rate) + '----'')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============================================== training validate ===============================================\n",
      "************** training **************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ryan/anaconda3/envs/tensorflow/lib/python3.6/site-packages/lightgbm/basic.py:1295: UserWarning: categorical_feature in Dataset is overridden.\n",
      "New categorical_feature is ['app_version', 'device_vendor', 'device_version', 'deviceid', 'lat', 'lng', 'lng_lat', 'netmodel', 'newsid', 'osversion', 'pos']\n",
      "  'New categorical_feature is {}'.format(sorted(list(categorical_feature))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds\n",
      "[50]\tvalid_0's auc: 0.967894\n",
      "[100]\tvalid_0's auc: 0.970435\n",
      "[150]\tvalid_0's auc: 0.971827\n",
      "[200]\tvalid_0's auc: 0.973224\n",
      "[250]\tvalid_0's auc: 0.974255\n",
      "[300]\tvalid_0's auc: 0.975138\n",
      "[350]\tvalid_0's auc: 0.97583\n",
      "[400]\tvalid_0's auc: 0.976445\n",
      "[450]\tvalid_0's auc: 0.976898\n",
      "[500]\tvalid_0's auc: 0.977326\n",
      "[550]\tvalid_0's auc: 0.977678\n",
      "[600]\tvalid_0's auc: 0.977983\n",
      "[650]\tvalid_0's auc: 0.978211\n"
     ]
    }
   ],
   "source": [
    "print('=============================================== training validate ===============================================')\n",
    "fea_imp_list = []\n",
    "clf = LGBMClassifier(\n",
    "    n_jobs=7,\n",
    "    learning_rate=0.01,\n",
    "    n_estimators=5000,\n",
    "    num_leaves=255,\n",
    "    subsample=0.9,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=2019,\n",
    "    metric=None\n",
    ")\n",
    "\n",
    "print('************** training **************')\n",
    "clf.fit(\n",
    "    train_x, train_y,\n",
    "    eval_set=[(val_x, val_y)],\n",
    "    eval_metric='auc',\n",
    "    categorical_feature=cate_cols,\n",
    "    early_stopping_rounds=200,\n",
    "    verbose=50,\n",
    "    callbacks=[learning_rate_callback]\n",
    ")\n",
    "print('runtime:', time.time() - t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "\n",
    "print('************** validate predict **************')\n",
    "best_rounds = clf.best_iteration_\n",
    "best_auc = clf.best_score_['valid_0']['auc']\n",
    "val_pred = clf.predict_proba(val_x)[:, 1]\n",
    "fea_imp_list.append(clf.feature_importances_)\n",
    "print('runtime:', time.time() - t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print('=============================================== training predict ===============================================')\n",
    "clf = LGBMClassifier(\n",
    "    learning_rate=0.01,\n",
    "    n_estimators=best_rounds,\n",
    "    num_leaves=255,\n",
    "    subsample=0.9,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=2019\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('************** training using all the data **************')\n",
    "clf.fit(\n",
    "    train_df, labels,\n",
    "    eval_set=[(train_df, labels)],\n",
    "    categorical_feature=cate_cols,\n",
    "    verbose=50\n",
    ")\n",
    "print('runtime:', time.time() - t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('************** test predict **************')\n",
    "# sub = pd.read_csv(path_data + 'sample.csv')\n",
    "\n",
    "sub['target'] = clf.predict_proba(test_df)[:, 1]\n",
    "clf.predict_proba(test_df)[:, 1]\n",
    "fea_imp_list.append(clf.feature_importances_)\n",
    "print('runtime:', time.time() - t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('=============================================== feat importances ===============================================')\n",
    "# 特征重要性可以好好看看\n",
    "fea_imp_dict = dict(zip(train_df.columns.values, np.mean(fea_imp_list, axis=0)))\n",
    "fea_imp_item = sorted(fea_imp_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "for f, imp in fea_imp_item:\n",
    "    print('{} = {}'.format(f, imp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('=============================================== threshold search ===============================================')\n",
    "# f1阈值敏感，所以对阈值做一个简单的迭代搜索。\n",
    "t0 = 0.05\n",
    "v = 0.002\n",
    "best_t = t0\n",
    "best_f1 = 0\n",
    "for step in range(201):\n",
    "    curr_t = t0 + step * v\n",
    "    y = [1 if x >= curr_t else 0 for x in val_pred]\n",
    "    curr_f1 = f1_score(val_y, y)\n",
    "    if curr_f1 > best_f1:\n",
    "        best_t = curr_t\n",
    "        best_f1 = curr_f1\n",
    "        print('step: {}   best threshold: {}   best f1: {}'.format(step, best_t, best_f1))\n",
    "print('search finish.')\n",
    "\n",
    "val_pred = [1 if x >= best_t else 0 for x in val_pred]\n",
    "print('\\nbest auc:', best_auc)\n",
    "print('best f1:', f1_score(val_y, val_pred))\n",
    "print('validate mean:', np.mean(val_pred))\n",
    "print('runtime:', time.time() - t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('=============================================== sub save ===============================================')\n",
    "sub.to_csv('sub_prob_{}_{}_{}.csv'.format(best_auc, best_f1, sub['target'].mean()), index=False)\n",
    "sub['target'] = sub['target'].apply(lambda x: 1 if x >= best_t else 0)\n",
    "sub.to_csv('sub_{}_{}_{}.csv'.format(best_auc, best_f1, sub['target'].mean()), index=False)\n",
    "print('runtime:', time.time() - t)\n",
    "print('finish.')\n",
    "print('========================================================================================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
